{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-02T01:06:44.070851Z",
     "start_time": "2025-04-02T01:06:23.654320Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Load Llama/Mistral model\n",
    "llm = Llama(model_path=\"/Users/jameelamer/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_ctx=4096)\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"OrganiZo is a digital asset management platform. It helps businesses organize files efficiently.\n",
    "Users can reset their password by clicking 'Forgot Password' on the login page.\n",
    "The platform supports multiple languages for better accessibility.\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "prompt = f\"\"\"\n",
    "Extract at least 3 important questions from the following text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Format the output as a list of questions only, without additional text.\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "response = llm(prompt)\n",
    "questions = response[\"choices\"][0][\"text\"].strip().split(\"\\n\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"question\": questions, \"answer\": [text] * len(questions)})\n",
    "\n",
    "# Save to CSV\n",
    "#df.to_csv(\"questions.csv\", index=False)\n",
    "df.head(10)\n",
    "print(\"Questions generated and saved to questions.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/jameelamer/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x156652a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x1047e91e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1047e60e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x1221ec990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1221f0460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x1221f0870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x1221ee850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x156004080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x103665a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1047e6780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x156652ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x1221edce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x156653370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1221f1c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x1047ea980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x156653850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x1221f24f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x15600d760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x156653e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x1221f2750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x126e04f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x1047eae60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x1221f3650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x15600e660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x15600ed50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1566540d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e05480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x1221f3cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1221f3f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x15600f160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1221f46c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x156654540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x1221f4a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x102f0dc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15600f790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e05800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x102f0a9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1566547a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15600fc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1221f5230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1221f56a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156654a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1221f5b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10365cf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1036690e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156010070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156654c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1560107c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1221f6280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156654ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1221f68c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156655120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x10364fcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x1221f71e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1221f7650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x1221f7ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e05d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156655380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e06460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1566555e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156655840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156655aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1221f7d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1221f7f80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156011740 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1047eb270 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1560119a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156655d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156655f60 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1221f89f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1566561c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1221f8c50 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156656420 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1221f8fd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1221f9230 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156656680 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1566568e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1221f99b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1221fa030 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156012110 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1047eb4d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1047eb730 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1221fa440 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1221fa9a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1221fb080 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10365bc80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1221fb820 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x102f1cfc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1221fba80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156012520 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156012780 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1221fbe90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156013010 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10365bee0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156656b40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156013270 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1221fc6d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1221fc930 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10365d930 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1047eb990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1221fd040 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1221fd3c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156656da0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1221fd7d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156657000 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156657260 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156013680 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156013da0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1221fe250 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1566574c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10365db90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1221fe4b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1047ebda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1047ec1b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1221fed90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1221ff340 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156014210 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1221ff780 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x162a04080 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1047ec410 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x162a04400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156657720 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x162a04db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x162a05010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1047ecb30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x162a05270 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x162a059a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x162a05f50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156014940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x162a061b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156657af0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x162a06530 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x162a068b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x162a06fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x162a07240 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x162a07970 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x162a07bd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x162a08300 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x162a08560 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x162a08b00 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e067e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156657d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156657fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x162a093e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156658220 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156015350 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x162a09640 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156658480 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x162a09c00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10365ddf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x162a0a130 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x162a0a390 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1560158e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156015c60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x162a0abf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1566588c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x162a0b060 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156658b20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156658fa0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x162a0b470 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x162a0bbf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x102f1d340 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x162a0c000 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x162a0c560 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156659200 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156659860 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156659ac0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x162a0cd60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10366b990 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x162a0d650 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x162a0d8b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10366c110 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x162a0de60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x162a0e590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1047ed510 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156659f00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x162a0e9a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15665a370 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15665a5d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x162a0ef40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156016c10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x162a0f2c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x162a0f640 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15665a830 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x162a0f9c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x162a0fc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x15665aa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x162a10540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x15665acf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1047ed9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x10366c370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x102f1d6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156017840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x162a10e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15665b760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x162a11070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x126e06a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x162a11790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x162a11b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x162a11e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15665bfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1047ede00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x102f1da40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x162a125a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10366cd10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x162a12b30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x162a12d90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10366cf70 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10366d490 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10366d6f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10366da70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x102f1ddc0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15665c7b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15665cf00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x162a13620 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10366ddf0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1560189b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x162a13880 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x162a140b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x162a145a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156018c10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15665d670 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x162a14800 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10366e2c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x162a14a60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1566495b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15665da80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15665dce0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156019960 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15665e2a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10366e520 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1047ee060 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10366e7b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x162a15420 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15665e6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15665e910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x162a15ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10366eca0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x162a16060 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15665eed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156019bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10366ef00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x162a162c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x162a169f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15665f490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x162a17280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15665fbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1047ee770 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x162a17b10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15665fe40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15601a430 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x1566602e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x162a180d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156660540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156660980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156660df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x162a18a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x162a18c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x162a18fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x162a19360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156661230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156661490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156661900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x162a19910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15601a840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x162a19ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x156661d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1047eebe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x162a1a240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x162a1a960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x156662320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x156662790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15601b0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1566629f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x162a1b280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x162a1baf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x162a1c360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x162a1cb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1566630b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x162a1cdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x162a1d020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156663670 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =    5545.78 ms\n",
      "llama_perf_context_print: prompt eval time =    5545.57 ms /    83 tokens (   66.81 ms per token,    14.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     951.30 ms /    15 runs   (   63.42 ms per token,    15.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    6499.90 ms /    98 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions generated and saved to questions.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:07:13.320105Z",
     "start_time": "2025-04-02T01:07:13.305852Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "b83cea1340536242",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               question                                             answer\n",
       "0  1. What is OrganiZo?  OrganiZo is a digital asset management platfor...\n",
       "1     2. How does Organ  OrganiZo is a digital asset management platfor..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. What is OrganiZo?</td>\n",
       "      <td>OrganiZo is a digital asset management platfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. How does Organ</td>\n",
       "      <td>OrganiZo is a digital asset management platfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:09:00.622031Z",
     "start_time": "2025-04-02T01:08:54.934998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "Users can reset their password by clicking 'Forgot Password' on the login page.\n",
    "The platform supports multiple languages for better accessibility.\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "prompt = f\"\"\"\n",
    "Extract at least 3 important questions from the following text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Format the output as a list of questions only, without additional text.\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "response = llm(prompt)\n",
    "questions = response[\"choices\"][0][\"text\"].strip().split(\"\\n\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"question\": questions, \"answer\": [text] * len(questions)})\n",
    "\n",
    "# Save to CSV\n",
    "#df.to_csv(\"questions.csv\", index=False)\n",
    "df.head(10)"
   ],
   "id": "efc731f421b3262a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 17 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5545.78 ms\n",
      "llama_perf_context_print: prompt eval time =    4528.06 ms /    48 tokens (   94.33 ms per token,    10.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.95 ms /    15 runs   (   75.40 ms per token,    13.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    5662.39 ms /    63 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                 question  \\\n",
       "0  1. How can users reset their password?   \n",
       "1                  2. What languages does   \n",
       "\n",
       "                                              answer  \n",
       "0  \\nUsers can reset their password by clicking '...  \n",
       "1  \\nUsers can reset their password by clicking '...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. How can users reset their password?</td>\n",
       "      <td>\\nUsers can reset their password by clicking '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. What languages does</td>\n",
       "      <td>\\nUsers can reset their password by clicking '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:14:15.682698Z",
     "start_time": "2025-04-02T01:14:15.214949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "text = \"\"\"Python is a programming language. What is Python used for? It is used for web development, AI, and more. Can I use Python for machine learning? Yes, it is widely used in ML. How do I install Python?\"\"\"\n",
    "\n",
    "def extract_questions(text):\n",
    "    sentences = text.split(\". \")\n",
    "    return [sent for sent in sentences if \"?\" in sent]\n",
    "\n",
    "questions = extract_questions(text)\n",
    "print(questions)"
   ],
   "id": "3e0094e4f42229d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:28:26.692963Z",
     "start_time": "2025-04-02T01:27:07.746223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the question generation model\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"iarfmoose/t5-base-question-generator\")\n",
    "\n",
    "def generate_questions(text, num_questions=5):\n",
    "    \"\"\"\n",
    "    Generates questions from input text.\n",
    "    \n",
    "    :param text: Input text for generating questions\n",
    "    :param num_questions: Number of questions to generate\n",
    "    :return: List of generated questions\n",
    "    \"\"\"\n",
    "    questions = qg_pipeline(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        num_return_sequences=num_questions,\n",
    "        num_beams=num_questions,  # Enables beam search\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    return [q[\"generated_text\"] for q in questions]\n",
    "\n",
    "# Example text (Replace this with any document content)\n",
    "text = \"\"\"\n",
    "Python is a widely-used programming language for web development, AI, data science, and automation. \n",
    "It was created by Guido van Rossum and first released in 1991. \n",
    "Python is known for its simplicity, readability, and large community support.\n",
    "\"\"\"\n",
    "\n",
    "# Generate questions\n",
    "questions = generate_questions(text)\n",
    "\n",
    "# Print the generated questions\n",
    "print(\"\\nGenerated Questions:\")\n",
    "for idx, q in enumerate(questions, 1):\n",
    "    print(f\"{idx}. {q}\")\n"
   ],
   "id": "3d38790b8923434c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Questions:\n",
      "1. What is Python? It is a popular programming language for web development, AI, and data science.\n",
      "2. What is Python? It is a popular programming language for web development, data science, and automation.\n",
      "3. What is a popular programming language for web development, AI, and automation?\n",
      "4. What is a popular programming language for web development, data science, and automation?\n",
      "5. What is the most popular programming language in the world?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:35:40.319479Z",
     "start_time": "2025-04-02T01:35:34.479585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load models\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"iarfmoose/t5-base-question-generator\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def generate_questions(text, num_questions=10):\n",
    "    \"\"\"\n",
    "    Generates meaningful questions from input text.\n",
    "    \"\"\"\n",
    "    questions = qg_pipeline(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        num_return_sequences=num_questions,\n",
    "        num_beams=num_questions,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return [q[\"generated_text\"] for q in questions]\n",
    "\n",
    "def answer_questions(text, questions):\n",
    "    \"\"\"\n",
    "    Answers generated questions based on input text.\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    for q in questions:\n",
    "        answer = qa_pipeline(question=q, context=text)\n",
    "        answers.append((q, answer[\"answer\"]))\n",
    "    return answers\n",
    "\n",
    "# Example text (Replace with document content)\n",
    "text = \"\"\"\n",
    "Python is a widely-used programming language for web development, AI, data science, and automation. \n",
    "It was created by Guido van Rossum and first released in 1991. \n",
    "Python is known for its simplicity, readability, and large community support.\n",
    "\"\"\"\n",
    "text = \"\"\"OrganiZo is a digital asset management platform. It helps businesses organize files efficiently.\n",
    "Users can reset their password by clicking 'Forgot Password' on the login page.\n",
    "The platform supports multiple languages for better accessibility.\"\"\"\n",
    "\n",
    "# Generate questions\n",
    "questions = generate_questions(text)\n",
    "\n",
    "# Get answers\n",
    "qa_pairs = answer_questions(text, questions)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nGenerated Questions & Answers:\")\n",
    "for idx, (q, a) in enumerate(qa_pairs, 1):\n",
    "    print(f\"{idx}. Q: {q}\\n   A: {a}\\n\")\n"
   ],
   "id": "85597752cbb2fdd8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Questions & Answers:\n",
      "1. Q: How can I reset my password? ? OrganiZo is a digital asset management platform.\n",
      "   A: by clicking 'Forgot Password' on the login page\n",
      "\n",
      "2. Q: What is OrganiZo? ? OrganiZo is a digital asset management platform.\n",
      "   A: It helps businesses organize files efficiently\n",
      "\n",
      "3. Q: How can I reset my password? ? OrganiZo helps businesses organize files efficiently.\n",
      "   A: by clicking 'Forgot Password' on the login page\n",
      "\n",
      "4. Q: What is OrganiZo? It is a digital asset management platform for businesses.\n",
      "   A: helps businesses organize files efficiently\n",
      "\n",
      "5. Q: is a digital asset management platform that helps businesses organize files efficiently.\n",
      "   A: OrganiZo\n",
      "\n",
      "6. Q: is a digital asset management platform that helps businesses organize files efficiently?\n",
      "   A: OrganiZo\n",
      "\n",
      "7. Q: What is OrganiZo? It is a digital asset management platform.\n",
      "   A: It helps businesses organize files efficiently\n",
      "\n",
      "8. Q: is a digital asset management platform that helps businesses organize their files efficiently.\n",
      "   A: OrganiZo\n",
      "\n",
      "9. Q: What is OrganiZo's password for??\n",
      "   A: Forgot Password\n",
      "\n",
      "10. Q: What is OrganiZo's password??\n",
      "   A: Forgot Password\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "221f8e68dd0d90f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
