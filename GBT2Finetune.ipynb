{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-09T09:32:41.951273Z",
     "start_time": "2025-03-09T09:32:40.936042Z"
    }
   },
   "source": "!pip install ArabertPreprocessor",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not find a version that satisfies the requirement ArabertPreprocessor (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for ArabertPreprocessor\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-09T09:32:51.574873Z",
     "start_time": "2025-03-09T09:32:45.694780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# finetune process \n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ],
   "id": "c9e43e1f16d0e37f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:33:10.517665Z",
     "start_time": "2025-03-09T09:33:08.089665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the data\n",
    "bbc_data = pd.read_csv('bbc_news_arabic_summarization.csv')\n",
    "# Randomly sample 50% of the dataset\n",
    "#bbc_data = bbc_data.sample(frac=0.5, random_state=42)"
   ],
   "id": "57dce74dad96fd3d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:34:34.795886Z",
     "start_time": "2025-03-09T09:34:34.788534Z"
    }
   },
   "cell_type": "code",
   "source": "bbc_data.head()",
   "id": "3560b6266e9792eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "0  140323_russian_troops_crimea_naval_base   \n",
       "1                    130528_egypt_nile_dam   \n",
       "2                           world-47242349   \n",
       "3                        vert-cul-55078328   \n",
       "4                     141023_yemen_hodeida   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.bbc.com/arabic/worldnews/2014/03/1...   \n",
       "1  https://www.bbc.com/arabic/middleeast/2013/05/...   \n",
       "2          https://www.bbc.com/arabic/world-47242349   \n",
       "3       https://www.bbc.com/arabic/vert-cul-55078328   \n",
       "4  https://www.bbc.com/arabic/middleeast/2014/10/...   \n",
       "\n",
       "                                               title  \\\n",
       "0           القوات الأوكرانية تبدأ الانسحاب من القرم   \n",
       "1    هل يفرض سد النهضة الإثيوبي واقعا جديدا على مصر؟   \n",
       "2  تعرف على منطقة كشمير التي تسببت بحربين بين اله...   \n",
       "3  ماذا تعرف عن العالم الخفي للمعابد اليابانية ال...   \n",
       "4  اشتباك بين الحوثيين و\"الحراك التهامي\" في الحدي...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  بدأت القوات الأوكرانية الانسحاب من شبه جزيرة ا...   \n",
       "1  \"هل سيتم تغيير العبارة الشهيرة للمؤرخ اليوناني...   \n",
       "2  قالت الشرطة في القطاع الهندي من إقليم كشمير إن...   \n",
       "3  في عام 816، تجول راهب يدعى كوكاي، في المنحدرات...   \n",
       "4  أكد مصدر في \"الحراك التهامي\" لأبناء محافظة الح...   \n",
       "\n",
       "                                                text  \n",
       "0  وكان الرئيس الأوكراني المؤقت، الكسندر تورتشينو...  \n",
       "1  بحلول عام 2050 ستحتاج مصر إلى 21 مليار متر مكع...  \n",
       "2  وذكرت وكالة الأنباء المحلية (جي.إن.إس) أن جماع...  \n",
       "3  ووقع اختياره على واد عمقه 800 متر محاط بثماني ...  \n",
       "4  مسلح حوثي في إب وقال المصدر إن المسلحين الحوثي...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140323_russian_troops_crimea_naval_base</td>\n",
       "      <td>https://www.bbc.com/arabic/worldnews/2014/03/1...</td>\n",
       "      <td>القوات الأوكرانية تبدأ الانسحاب من القرم</td>\n",
       "      <td>بدأت القوات الأوكرانية الانسحاب من شبه جزيرة ا...</td>\n",
       "      <td>وكان الرئيس الأوكراني المؤقت، الكسندر تورتشينو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130528_egypt_nile_dam</td>\n",
       "      <td>https://www.bbc.com/arabic/middleeast/2013/05/...</td>\n",
       "      <td>هل يفرض سد النهضة الإثيوبي واقعا جديدا على مصر؟</td>\n",
       "      <td>\"هل سيتم تغيير العبارة الشهيرة للمؤرخ اليوناني...</td>\n",
       "      <td>بحلول عام 2050 ستحتاج مصر إلى 21 مليار متر مكع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world-47242349</td>\n",
       "      <td>https://www.bbc.com/arabic/world-47242349</td>\n",
       "      <td>تعرف على منطقة كشمير التي تسببت بحربين بين اله...</td>\n",
       "      <td>قالت الشرطة في القطاع الهندي من إقليم كشمير إن...</td>\n",
       "      <td>وذكرت وكالة الأنباء المحلية (جي.إن.إس) أن جماع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vert-cul-55078328</td>\n",
       "      <td>https://www.bbc.com/arabic/vert-cul-55078328</td>\n",
       "      <td>ماذا تعرف عن العالم الخفي للمعابد اليابانية ال...</td>\n",
       "      <td>في عام 816، تجول راهب يدعى كوكاي، في المنحدرات...</td>\n",
       "      <td>ووقع اختياره على واد عمقه 800 متر محاط بثماني ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141023_yemen_hodeida</td>\n",
       "      <td>https://www.bbc.com/arabic/middleeast/2014/10/...</td>\n",
       "      <td>اشتباك بين الحوثيين و\"الحراك التهامي\" في الحدي...</td>\n",
       "      <td>أكد مصدر في \"الحراك التهامي\" لأبناء محافظة الح...</td>\n",
       "      <td>مسلح حوثي في إب وقال المصدر إن المسلحين الحوثي...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:34:38.416360Z",
     "start_time": "2025-03-09T09:34:38.357791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bbc_data.dropna(subset=['text', 'summary'], inplace=True)\n",
    "bbc_data = bbc_data.drop(columns=['id','url','title','summary'])"
   ],
   "id": "c2cec1daa08eea41",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:35:04.584433Z",
     "start_time": "2025-03-09T09:35:03.065628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "bbc_dataset= Dataset.from_pandas(bbc_data)\n",
    "train_test_split = bbc_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "bbc_train_dataset1 = train_test_split['train']\n",
    "bbc_test_dataset = train_test_split['test']\n",
    "# Split the training set further into 80% train and 20% validation\n",
    "bbc_train_dataset, bbc_val_dataset = bbc_train_dataset1.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n"
   ],
   "id": "533a49cbf4a5dda9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:42:31.817058Z",
     "start_time": "2025-03-09T09:41:35.261519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "# Initialize AraBERT preprocessor\n",
    "arabert_prep = ArabertPreprocessor(\"aubmindlab/aragpt2-base\")\n",
    "# Preprocess dataset for Arabic\n",
    "def preprocess_arabic(examples):\n",
    "    examples[\"text\"] = [arabert_prep.preprocess(text) for text in examples[\"text\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = bbc_train_dataset.map(preprocess_arabic, batched=True)\n",
    "val_dataset = bbc_val_dataset.map(preprocess_arabic, batched=True)\n",
    "test_dataset = bbc_test_dataset.map(preprocess_arabic, batched=True)\n",
    "# Load GPT-2 model and tokenizer for Arabic\n",
    "model_name = \"aubmindlab/aragpt2-base\"  # Pre-trained Arabic GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "aa083fef65f6d9fd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30013/30013 [00:35<00:00, 839.65 examples/s]\n",
      "Map: 100%|██████████| 7504/7504 [00:08<00:00, 863.98 examples/s]\n",
      "Map: 100%|██████████| 9380/9380 [00:10<00:00, 861.21 examples/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:45:06.953938Z",
     "start_time": "2025-03-09T09:45:06.948747Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_dataset.column_names)",
   "id": "79bb2924ccead2ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:47:07.501967Z",
     "start_time": "2025-03-09T09:45:49.961270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize dataset\n",
    "# Set the pad token to <|endoftext|>\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "#tokenized_datasets = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])   \"__index_level_0__\"\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal language modeling\n",
    ")\n",
    "print(train_dataset.column_names)\n"
   ],
   "id": "10cafcf778d1da15",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30013/30013 [00:59<00:00, 506.49 examples/s]\n",
      "Map: 100%|██████████| 7504/7504 [00:17<00:00, 439.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "426ea66247c216ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T09:52:39.781499Z",
     "start_time": "2025-03-09T09:52:39.683235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./SSS_arabic_gpt2_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,  # Adjust for CPU\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=True,\n",
    "    remove_unused_columns=False,  # Prevent Trainer from filtering columns\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ],
   "id": "cdacc378c995ab83",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Training arguments\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m training_args = \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./SSS_arabic_gpt2_finetuned\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mepoch\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Adjust for CPU\u001B[39;49;00m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./logs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_total_limit\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mepoch\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cpu\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mremove_unused_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Prevent Trainer from filtering columns\u001B[39;49;00m\n\u001B[32m     16\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Trainer setup\u001B[39;00m\n\u001B[32m     19\u001B[39m trainer = Trainer(\n\u001B[32m     20\u001B[39m     model=model,\n\u001B[32m     21\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     25\u001B[39m     data_collator=data_collator,\n\u001B[32m     26\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:131\u001B[39m, in \u001B[36m__init__\u001B[39m\u001B[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, eval_use_gather_object)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/training_args.py:1730\u001B[39m, in \u001B[36m__post_init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/training_args.py:2227\u001B[39m, in \u001B[36mdevice\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2225\u001B[39m     torch.cuda.set_device(device)\n\u001B[32m   2226\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_sagemaker_dp_enabled():\n\u001B[32m-> \u001B[39m\u001B[32m2227\u001B[39m     accelerator_state_kwargs[\u001B[33m\"\u001B[39m\u001B[33m_use_sagemaker_dp\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   2228\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.deepspeed:\n\u001B[32m   2229\u001B[39m     accelerator_state_kwargs[\u001B[33m\"\u001B[39m\u001B[33muse_deepspeed\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:60\u001B[39m, in \u001B[36m__get__\u001B[39m\u001B[34m(self, obj, objtype)\u001B[39m\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33munreadable attribute\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     59\u001B[39m attr = \u001B[33m\"\u001B[39m\u001B[33m__cached_\u001B[39m\u001B[33m\"\u001B[39m + \u001B[38;5;28mself\u001B[39m.fget.\u001B[34m__name__\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m cached = \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     62\u001B[39m     cached = \u001B[38;5;28mself\u001B[39m.fget(obj)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/training_args.py:2103\u001B[39m, in \u001B[36m_setup_devices\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2096\u001B[39m         warnings.warn(\n\u001B[32m   2097\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m`--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2098\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m`--hub_model_id` instead and pass the full repo name to this argument (in this case \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2099\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.hub_model_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m).\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2100\u001B[39m             \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m   2101\u001B[39m         )\n\u001B[32m   2102\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.push_to_hub_organization \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2103\u001B[39m     \u001B[38;5;28mself\u001B[39m.hub_model_id = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.push_to_hub_organization\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPath(\u001B[38;5;28mself\u001B[39m.output_dir).name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   2104\u001B[39m     warnings.warn(\n\u001B[32m   2105\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m`--push_to_hub_organization` is deprecated and will be removed in version 5 of 🤗 Transformers. Use \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2106\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m`--hub_model_id` instead and pass the full repo name to this argument (in this case \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2107\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.hub_model_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m).\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2108\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m   2109\u001B[39m     )\n\u001B[32m   2111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.eval_use_gather_object \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available(\u001B[33m\"\u001B[39m\u001B[33m0.30.0\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[31mImportError\u001B[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T04:00:28.671954Z",
     "start_time": "2025-01-02T13:43:14.357969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "5ac1aa74b273f273",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11256' max='11256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11256/11256 14:17:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.275600</td>\n",
       "      <td>3.719555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>3.648158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.894000</td>\n",
       "      <td>3.626880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11256, training_loss=4.154550298868381, metrics={'train_runtime': 51434.1435, 'train_samples_per_second': 0.875, 'train_steps_per_second': 0.219, 'total_flos': 1.1762844696576e+16, 'train_loss': 4.154550298868381, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T16:25:12.259469Z",
     "start_time": "2025-01-06T16:25:12.099283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./arabic_gpt2_finetuned\")\n",
    "tokenizer.save_pretrained(\"./arabic_gpt2_finetuned\")"
   ],
   "id": "3f7168d6de2f4b0c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Save the fine-tuned model and tokenizer\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./arabic_gpt2_finetuned\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./arabic_gpt2_finetuned\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5bd8afd80bcb8a49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T10:29:19.334682Z",
     "start_time": "2025-01-04T10:29:19.247042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"قطر\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True).to(\"cpu\")\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ],
   "id": "2cbbfb03501d1b8c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m input_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mقطر\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mencode(input_text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Generate text\u001B[39;00m\n\u001B[1;32m      5\u001B[0m output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m      6\u001B[0m     input_ids,\n\u001B[1;32m      7\u001B[0m     max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m     early_stopping\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     11\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e66f7c52c3b2bb9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b44fbea282c52131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T06:18:36.487638Z",
     "start_time": "2025-01-03T06:18:36.220555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./smart_soft_arabic_gpt2\")\n",
    "tokenizer.save_pretrained(\"./smart_soft_arabic_gpt2\")"
   ],
   "id": "79d0427ca73932d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./smart_soft_arabic_gpt2/tokenizer_config.json',\n",
       " './smart_soft_arabic_gpt2/special_tokens_map.json',\n",
       " './smart_soft_arabic_gpt2/vocab.json',\n",
       " './smart_soft_arabic_gpt2/merges.txt',\n",
       " './smart_soft_arabic_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6c1ac35e1ea8790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:54:47.032427Z",
     "start_time": "2025-03-15T07:54:41.965931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from arabert.preprocess import ArabertPreprocessor\n"
   ],
   "id": "a52e72bab95653f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:54:56.182245Z",
     "start_time": "2025-03-15T07:54:56.089469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load GPT-2 model and tokenizer for Arabic\n",
    "model_name = \"./sss-arabic_gpt2_finetuned\"  # Pre-trained Arabic GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ],
   "id": "ff12ce1d5e760553",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:55:06.917115Z",
     "start_time": "2025-03-15T07:54:58.118092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text =\"الحرب السورية\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True).to(\"cpu\")\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ],
   "id": "e2a842f8e2101855",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "الحرب السورية مستمرة منذ أكثر من ثلاث سنوات وقال المرصد السوري لحقوق الإنسان المعارض ، ومقره بريطانيا ، إن الغارات الجوية التي يشنها التحالف الدولي بقيادة الولايات المتحدة على مواقع تنظيم \" الدولة الإسلامية \" في محافظة دير الزور أسفرت عن مقتل العشرات من مسلحي التنظيم . وأضاف المرصد أن الغارات أسفرت أيضا عن تدمير عدد من المنازل في المنطقة . وقال رامي عبد الرحمن مدير المرصد ، الذي يتخذ من بريطانيا مقرا له ، لبي بي سي إن الضربات الجوية استهدفت مواقع للتنظيم في مدينة البوكمال ، شرقي سوريا . مواضيع قد تهمك نهاية وأضاف أن طائرات التحالف شنت أيضا غارات على مناطق أخرى في سوريا ، بما في ذلك مدينة الرقة ، التي يسيطر عليها مسلحو التنظيم ، بحسب المرصد . وأشار المرصد إلى أن غارات التحالف استهدفت أيضا مواقع لمسلحي التنظيم في الرقة ودير الزور . وكانت القوات الحكومية السورية ، مدعومة بغطاء جوي روسي ، قد شنت في وقت سابق من الشهر الجاري ، عملية عسكرية لاستعادة السيطرة على مدينة تدمر الأثرية ، الواقعة على بعد 50 كيلومترا غرب العاصمة السورية دمشق ، والتي تعد من أهم المواقع الأثرية في العالم . وكان التنظيم قد سيطر على تدمر في يونيو - حزيران\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
