{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T05:57:52.425962Z",
     "start_time": "2025-03-28T05:57:47.181113Z"
    }
   },
   "source": [
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sympy import false\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from rouge_score import rouge_scorer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:57:55.685817Z",
     "start_time": "2025-03-28T05:57:55.544041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download necessary NLP tools\n",
    "nltk.download('punkt')\n"
   ],
   "id": "53c75ba6a23539b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jameelamer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:32:52.519955Z",
     "start_time": "2025-03-28T06:32:50.229549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set device (MPS if available, otherwise use CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)"
   ],
   "id": "7c013b2ceca271ef",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:58:45.387899Z",
     "start_time": "2025-03-28T05:58:45.326033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load BBC News Dataset \n",
    "df = pd.read_csv(\"bbc_news_summary_with_articles.csv\")"
   ],
   "id": "a135e3c47e282be3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:58:53.184396Z",
     "start_time": "2025-03-28T05:58:53.179728Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "e24d6ae86a9ccbe9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Title                                            Article  \\\n",
       "0    289  Musicians to tackle US red tape\\n\\nMusicians' ...   \n",
       "1    262  U2's desire to be number one\\n\\nU2, who have w...   \n",
       "2    276  Rocker Doherty in on-stage fight\\n\\nRock singe...   \n",
       "3     60  Snicket tops US box office chart\\n\\nThe film a...   \n",
       "4     74  Ocean's Twelve raids box office\\n\\nOcean's Twe...   \n",
       "\n",
       "                                             Summary       Category  \n",
       "0  Nigel McCune from the Musicians' Union said Br...  entertainment  \n",
       "1  But they still want more.They have to want to ...  entertainment  \n",
       "2  Babyshambles, which he formed after his acrimo...  entertainment  \n",
       "3  A Series of Unfortunate Events also stars Scot...  entertainment  \n",
       "4  Ocean's Twelve, the crime caper sequel starrin...  entertainment  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289</td>\n",
       "      <td>Musicians to tackle US red tape\\n\\nMusicians' ...</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262</td>\n",
       "      <td>U2's desire to be number one\\n\\nU2, who have w...</td>\n",
       "      <td>But they still want more.They have to want to ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276</td>\n",
       "      <td>Rocker Doherty in on-stage fight\\n\\nRock singe...</td>\n",
       "      <td>Babyshambles, which he formed after his acrimo...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>Snicket tops US box office chart\\n\\nThe film a...</td>\n",
       "      <td>A Series of Unfortunate Events also stars Scot...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>Ocean's Twelve raids box office\\n\\nOcean's Twe...</td>\n",
       "      <td>Ocean's Twelve, the crime caper sequel starrin...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:58:54.760422Z",
     "start_time": "2025-03-28T05:58:54.757283Z"
    }
   },
   "cell_type": "code",
   "source": "df=df.sample(frac=0.05, replace=True, random_state=1)",
   "id": "7257b6a7e9eeca9e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:58:56.430598Z",
     "start_time": "2025-03-28T05:58:56.427737Z"
    }
   },
   "cell_type": "code",
   "source": "df.size",
   "id": "525cb70e8c262fa2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:59:00.628266Z",
     "start_time": "2025-03-28T05:59:00.622446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    return nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "# Generate sentence embeddings\n",
    "def get_sentence_embeddings(sentences):\n",
    "    embeddings = sbert_model.encode(sentences, convert_to_tensor=True)\n",
    "    return embeddings.to(device)  # Ensure embeddings are moved to the same device\n",
    "\n",
    "# Define Reinforcement Learning Agent\n",
    "class RLAgent(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_sentences):\n",
    "        super(RLAgent, self).__init__()\n",
    "        self.fc = torch.nn.Linear(embedding_dim, num_sentences).to(device)  # Move to device\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        scores = self.fc(embeddings)\n",
    "        return self.softmax(scores)\n",
    "\n",
    "# Reward Function (ROUGE + Diversity Score)\n",
    "def reward_function(selected_sentences, reference_summary):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    rouge_scores = np.mean([scorer.score(reference_summary, sent)['rouge1'].fmeasure for sent in selected_sentences])\n",
    "    diversity_score = len(set(selected_sentences)) / max(1, len(selected_sentences))\n",
    "    return rouge_scores + 0.5 * diversity_score  # Weighted sum\n",
    "\n",
    "def compute_reward(generated_summary, reference_summary):\n",
    "    rouge_score = compute_rouge(generated_summary, reference_summary)\n",
    "    diversity_score = compute_diversity(generated_summary)\n",
    "    readability_score = compute_readability(generated_summary)\n",
    "    return 0.5 * rouge_score + 0.3 * diversity_score + 0.2 * readability_score\n"
   ],
   "id": "85a654c30f6cef37",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:59:05.798496Z",
     "start_time": "2025-03-28T05:59:05.792937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train RL Agent\n",
    "def train_rl_agent(df, num_epochs=3, learning_rate=0.01):\n",
    "    agent = RLAgent(384, 10).to(device)  # Ensure model runs on GPU if available\n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_reward = 0\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            article, reference_summary = row['Article'], row['Summary']\n",
    "            sentences = preprocess_text(article)\n",
    "            embeddings = get_sentence_embeddings(sentences)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            probs = agent(embeddings)  # Get probability scores for each sentence\n",
    "            probs = probs.squeeze()  # Remove extra dimensions if needed\n",
    "\n",
    "            # âœ… Ensure probabilities are valid (Avoids division errors)\n",
    "            if probs.numel() == 0 or torch.all(probs == 0):\n",
    "                print(\"Skipping due to empty or zero probability distribution.\")\n",
    "                continue\n",
    "            \n",
    "            # âœ… Ensure num_samples is within a valid range\n",
    "            num_sentences = len(sentences)\n",
    "            num_samples = min(max(num_sentences // 3, 1), len(probs))\n",
    "\n",
    "            # âœ… Check if we have enough elements to sample\n",
    "            # if num_samples <= len(probs):  \n",
    "            #     chosen_indices = torch.multinomial(probs, num_samples=num_samples, replacement=False)\n",
    "            # else:\n",
    "            #     chosen_indices = torch.arange(len(probs))  # Select all available indices\n",
    "            chosen_indices = torch.arange(len(probs))\n",
    "            # âœ… Convert chosen indices to a flat list\n",
    "            chosen_indices = chosen_indices.cpu().numpy().flatten().tolist()\n",
    "            selected_sentences = [sentences[i] for i in chosen_indices]\n",
    "            \n",
    "            reward = reward_function(selected_sentences, reference_summary)\n",
    "            loss = -torch.log(probs[chosen_indices] + 1e-8).sum() * reward  # Avoid log(0)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_reward += reward\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Avg Reward = {total_reward / len(df):.4f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n"
   ],
   "id": "34bdd6a8803cc6bd",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:14:30.010688Z",
     "start_time": "2025-03-27T11:02:51.597433Z"
    }
   },
   "cell_type": "code",
   "source": "trained_agent = train_rl_agent(df)",
   "id": "313c3f168b37c07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2225/2225 [06:17<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Reward = 0.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2225/2225 [02:40<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Avg Reward = 0.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2225/2225 [02:40<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Avg Reward = 0.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:24:36.968798Z",
     "start_time": "2025-03-27T11:24:36.957139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate model performance\n",
    "def evaluate_model(agent, df,type):\n",
    "    total_rouge_score = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        article, reference_summary = row['Article'], row['Summary']\n",
    "        sentences = preprocess_text(article)\n",
    "        embeddings = get_sentence_embeddings(sentences)\n",
    "\n",
    "        probs = agent(embeddings).squeeze()  # Get probability scores\n",
    "        probs = probs.detach().cpu()  # Move to CPU for processing\n",
    "\n",
    "        # âœ… Ensure probabilities are valid\n",
    "        if probs.numel() == 0 or torch.all(probs == 0):\n",
    "            print(\"Skipping due to empty or zero probability distribution.\")\n",
    "            continue\n",
    "\n",
    "        # âœ… Ensure valid number of sentences\n",
    "        num_samples = min(max(len(sentences) // 3, 1), len(probs))\n",
    "\n",
    "        # âœ… Get selected sentence indices\n",
    "        # if num_samples <= len(probs):  \n",
    "        #     chosen_indices = torch.multinomial(probs, num_samples=num_samples, replacement=False)\n",
    "        # else:\n",
    "        chosen_indices = torch.arange(len(probs))  # Select all available indices\n",
    "\n",
    "        # âœ… Convert `chosen_indices` to a **flat list of integers**\n",
    "        chosen_indices = chosen_indices.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        # âœ… Fix: Ensure `chosen_indices` are integers\n",
    "        selected_sentences = [sentences[int(i)] for i in chosen_indices]  # Convert to int before indexing\n",
    "\n",
    "        # Compute ROUGE score\n",
    "        generated_summary = \" \".join(selected_sentences)\n",
    "        rouge_score = compute_rouge(generated_summary, reference_summary,type)\n",
    "        total_rouge_score += rouge_score\n",
    "\n",
    "    avg_rouge = total_rouge_score / len(df)\n",
    "    print(f\"Average ROUGE Score {type}: {avg_rouge:.4f}\")\n",
    "    return avg_rouge\n"
   ],
   "id": "46cb84c0a55dccee",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:24:40.116536Z",
     "start_time": "2025-03-27T11:24:40.109286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def compute_rouge(generated_summary, reference_summary,type):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "    # Compute the average ROUGE score\n",
    "    rouge1 = scores['rouge1'].fmeasure\n",
    "    rouge2 = scores['rouge2'].fmeasure\n",
    "    rougeL = scores['rougeL'].fmeasure\n",
    "    avg_rouge = (rouge1 + rouge2 + rougeL) / 3  # Average ROUGE score\n",
    "    if(type==\"rouge1\"):\n",
    "        return rouge1\n",
    "    if(type==\"rouge2\"):\n",
    "        return rouge2\n",
    "    if(type==\"rougeL\"):\n",
    "        return rougeL\n",
    "    \n",
    "    return avg_rouge\n"
   ],
   "id": "6e79536b616067ca",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:31:04.633136Z",
     "start_time": "2025-03-27T11:24:44.535581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluate_model(trained_agent, df,\"rouge1\")\n",
    "evaluate_model(trained_agent, df, \"rouge2\")\n",
    "evaluate_model(trained_agent, df, \"rougeL\")\n",
    "evaluate_model(trained_agent, df, \"\")"
   ],
   "id": "87cf7ae7dcc1ee5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2225/2225 [01:35<00:00, 23.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Score rouge1: 0.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2225/2225 [01:34<00:00, 23.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Score rouge2: 0.5949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2225/2225 [01:35<00:00, 23.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Score rougeL: 0.3843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2225/2225 [01:34<00:00, 23.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Score : 0.5318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.531794009110535"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8c60569147a19958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# finetune SBERT    ",
   "id": "7a8b3e2db6a3565"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:46:37.173186Z",
     "start_time": "2025-03-28T06:46:37.167908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer"
   ],
   "id": "82997d889113f501",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:46:43.745574Z",
     "start_time": "2025-03-28T06:46:43.740170Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "1c6e3ddc9bd89af0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Title                                            Article  \\\n",
       "1061    405  Ireland 19-13 England\\n\\nIreland consigned Eng...   \n",
       "235     352  Vera Drake's Bafta triumph hope\\n\\nAt the Baft...   \n",
       "1096     38  Radcliffe proves doubters wrong\\n\\nThis won't ...   \n",
       "905     100  Mido makes third apology\\n\\nAhmed 'Mido' Hossa...   \n",
       "960     502  Minister digs in over doping row\\n\\nThe Belgia...   \n",
       "\n",
       "                                                Summary       Category  \n",
       "1061  O'Gara missed a penalty which would have put I...          sport  \n",
       "235   \"If Mike Leigh is going to win awards for anyt...  entertainment  \n",
       "1096  And a lot of people were wondering what would ...          sport  \n",
       "905   Shalaby earlier said that after an apology Mid...          sport  \n",
       "960   Dechy said.The Belgian sports minister at the ...          sport  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>405</td>\n",
       "      <td>Ireland 19-13 England\\n\\nIreland consigned Eng...</td>\n",
       "      <td>O'Gara missed a penalty which would have put I...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>352</td>\n",
       "      <td>Vera Drake's Bafta triumph hope\\n\\nAt the Baft...</td>\n",
       "      <td>\"If Mike Leigh is going to win awards for anyt...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>38</td>\n",
       "      <td>Radcliffe proves doubters wrong\\n\\nThis won't ...</td>\n",
       "      <td>And a lot of people were wondering what would ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>100</td>\n",
       "      <td>Mido makes third apology\\n\\nAhmed 'Mido' Hossa...</td>\n",
       "      <td>Shalaby earlier said that after an apology Mid...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>502</td>\n",
       "      <td>Minister digs in over doping row\\n\\nThe Belgia...</td>\n",
       "      <td>Dechy said.The Belgian sports minister at the ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:46:46.585682Z",
     "start_time": "2025-03-28T06:46:46.582078Z"
    }
   },
   "cell_type": "code",
   "source": "train_df=df",
   "id": "a2e3882602b4657b",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:46:47.636650Z",
     "start_time": "2025-03-28T06:46:47.633102Z"
    }
   },
   "cell_type": "code",
   "source": "train_df=df[['Article', 'Summary']]",
   "id": "f890473cac26b1d5",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T07:07:07.387119Z",
     "start_time": "2025-03-28T07:07:07.360017Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.dropna(subset=['Article', 'Summary'], inplace=True)",
   "id": "b561d5e97b6d8d78",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tq/qbd06jwj7cqgljch9ysfjt8w0000gn/T/ipykernel_1718/2257929473.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(subset=['Article', 'Summary'], inplace=True)\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T07:46:13.679508Z",
     "start_time": "2025-03-28T07:46:13.174430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import nltk\n",
    "import os\n",
    "# Download NLTK packages\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Set the environment variable to allow more memory usage on MPS (Metal Performance Shaders)\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "# Load BBC News dataset (replace with actual dataset path if local)\n",
    "dataset = load_dataset(\"csv\", data_files=\"bbc_news_summary_with_articles.csv\")\n"
   ],
   "id": "8dc0c699b9177989",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jameelamer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T10:03:46.373247Z",
     "start_time": "2025-03-28T07:56:14.083481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split dataset into train/test\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Load pre-trained BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the input articles\n",
    "    model_inputs = tokenizer(examples[\"Article\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Tokenize the summaries as target\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"Summary\"], max_length=100, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Add labels to the inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the BART model for summarization\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Set device to MPS or CPU\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")  # Use MPS if available, otherwise fallback to CPU\n",
    "model.to(device)\n",
    "\n",
    "# Define training arguments with optimizations for memory and ensuring no fp16 usage\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart_summarization\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,  # Reduce batch size to fit in memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients over multiple steps\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=False,  # Ensure fp16 is disabled to avoid issues with MPS\n",
    "    no_cuda=True,  # Disable CUDA (which is for GPU) completely\n",
    ")\n",
    "\n",
    "# Data collator for Seq2Seq tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ],
   "id": "65bfd727c7ccae76",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1312 [00:00<?, ? examples/s]/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1312/1312 [00:03<00:00, 349.91 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [00:00<00:00, 307.29 examples/s]\n",
      "/var/folders/tq/qbd06jwj7cqgljch9ysfjt8w0000gn/T/ipykernel_1718/4236310397.py:30: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")  # Use MPS if available, otherwise fallback to CPU\n",
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  loaded_dict = _convert_str_dict(loaded_dict)\n",
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  \n",
      "/var/folders/tq/qbd06jwj7cqgljch9ysfjt8w0000gn/T/ipykernel_1718/4236310397.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='492' max='492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [492/492 2:06:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.320352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.303818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=492, training_loss=0.31143762231842287, metrics={'train_runtime': 7631.6303, 'train_samples_per_second': 0.516, 'train_steps_per_second': 0.064, 'total_flos': 4264861856956416.0, 'train_loss': 0.31143762231842287, 'epoch': 3.0})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T10:10:10.815282Z",
     "start_time": "2025-03-28T10:10:08.824763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./bert_summarization_finetune/bbc_bart_summarization\")\n",
    "tokenizer.save_pretrained(\"./bert_summarization_finetune/bbc_bart_summarization\")"
   ],
   "id": "675286f82dfe32d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_summarization_finetune/bbc_bart_summarization/tokenizer_config.json',\n",
       " './bert_summarization_finetune/bbc_bart_summarization/special_tokens_map.json',\n",
       " './bert_summarization_finetune/bbc_bart_summarization/vocab.json',\n",
       " './bert_summarization_finetune/bbc_bart_summarization/merges.txt',\n",
       " './bert_summarization_finetune/bbc_bart_summarization/added_tokens.json')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:24:13.353448Z",
     "start_time": "2025-03-28T19:24:13.345194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to generate summaries\n",
    "def generate_summary(article_text, model, tokenizer):\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs.input_ids, max_length=350, min_length=50, length_penalty=2.0, num_beams=4)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ],
   "id": "7d4d0f4782e189b8",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:24:24.888413Z",
     "start_time": "2025-03-28T19:24:14.348499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#tokenizer = BartTokenizer.from_pretrained(\"./bert_summarization_finetune/bbc_bart_summarization\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# Test on a sample article from the test dataset\n",
    "sample_article = dataset[\"train\"][0][\"Article\"]\n",
    "sample_summary=  dataset[\"train\"][0][\"Summary\"]\n",
    "generated_summary = generate_summary(sample_article, model, tokenizer)\n",
    "\n",
    "print(\"Original Article:\", sample_article)\n",
    "print(\"\\nGenerated Summary:\", generated_summary)\n",
    "print(\"\\nSample Summary:\", sample_summary)"
   ],
   "id": "a1585aef3dd49e4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Musicians to tackle US red tape\n",
      "\n",
      "Musicians' groups are to tackle US visa regulations which are blamed for hindering British acts' chances of succeeding across the Atlantic.\n",
      "\n",
      "A singer hoping to perform in the US can expect to pay $1,300 (Â£680) simply for obtaining a visa. Groups including the Musicians' Union are calling for an end to the \"raw deal\" faced by British performers. US acts are not faced with comparable expense and bureaucracy when visiting the UK for promotional purposes.\n",
      "\n",
      "Nigel McCune from the Musicians' Union said British musicians are \"disadvantaged\" compared to their US counterparts. A sponsor has to make a petition on their behalf, which is a form amounting to nearly 30 pages, while musicians face tougher regulations than athletes and journalists. \"If you make a mistake on your form, you risk a five-year ban and thus the ability to further your career,\" says Mr McCune.\n",
      "\n",
      "\"The US is the world's biggest music market, which means something has to be done about the creaky bureaucracy,\" says Mr McCune. \"The current situation is preventing British acts from maintaining momentum and developing in the US,\" he added.\n",
      "\n",
      "The Musicians' Union stance is being endorsed by the Music Managers' Forum (MMF), who say British artists face \"an uphill struggle\" to succeed in the US, thanks to the tough visa requirements, which are also seen as impractical. The MMF's general secretary James Seller said: \"Imagine if you were an orchestra from the Orkneys? Every member would have to travel to London to have their visas processed.\"\n",
      "\n",
      "\"The US market is seen as the holy grail and one of the benchmarks of success, and we're still going to fight to get in there. \"It's still very important, but there are other markets like Europe, India and China,\" added Mr Seller. A Department for Media, Culture and Sport spokeswoman said: \"We're aware that people are experiencing problems, and are working with the US embassy and record industry to see what we can do about it.\" A US Embassy spokesman said: \"We are aware that entertainers require visas for time-specific visas and are doing everything we can to process those applications speedily.\" \"We are aware of the importance of cultural exchange and we will do our best to facilitate that,\" he added.\n",
      "\n",
      "\n",
      "Generated Summary: Musicians' groups call for an end to the \"raw deal\" faced by British performers. Musicians face tougher regulations than athletes and journalists. US acts are not faced with comparable expense and bureaucracy when visiting the UK for promotional purposes. Department for Media, Culture and Sport says it is working with US embassy and record industry.\n",
      "\n",
      "Sample Summary: Nigel McCune from the Musicians' Union said British musicians are \"disadvantaged\" compared to their US counterparts.A US Embassy spokesman said: \"We are aware that entertainers require visas for time-specific visas and are doing everything we can to process those applications speedily.\"The Musicians' Union stance is being endorsed by the Music Managers' Forum (MMF), who say British artists face \"an uphill struggle\" to succeed in the US, thanks to the tough visa requirements, which are also seen as impractical.Musicians' groups are to tackle US visa regulations which are blamed for hindering British acts' chances of succeeding across the Atlantic.\"The US is the world's biggest music market, which means something has to be done about the creaky bureaucracy,\" says Mr McCune.\"The current situation is preventing British acts from maintaining momentum and developing in the US,\" he added.A singer hoping to perform in the US can expect to pay $1,300 (Â£680) simply for obtaining a visa.\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T18:59:01.620196Z",
     "start_time": "2025-03-28T10:56:48.553963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load the BBC news dataset (ensure it's correctly loaded)\n",
    "dataset = load_dataset(\"csv\", data_files=\"bbc_news_summary_with_articles.csv\")\n",
    "# Load the pre-trained BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"./bert_summarization_finetune/bbc_bart_summarization\")\n",
    "# Load the fine-tuned model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"./bert_summarization_finetune/bbc_bart_summarization\")\n",
    "# Se\n",
    "# Set the device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Preprocess function for tokenizing input articles\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"Article\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"Summary\"], max_length=100, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset (only for evaluation)\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "# Function to generate summaries using the model\n",
    "def generate_summary(article_text, model, tokenizer):\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = inputs.to(device)  # Ensure the inputs are on the CPU\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=50, length_penalty=2.0, num_beams=4)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Evaluate manually and compute ROUGE scores\n",
    "def evaluate_model(dataset, model, tokenizer):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for article, summary in zip(dataset[\"Article\"], dataset[\"Summary\"]):\n",
    "        generated_summary = generate_summary(article, model, tokenizer)\n",
    "        predictions.append(generated_summary)\n",
    "        references.append(summary)\n",
    "    \n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge_scores[\"rouge1\"].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores[\"rouge2\"].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores[\"rougeL\"].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    # Average the scores\n",
    "    avg_scores = {key: sum(value) / len(value) for key, value in rouge_scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Evaluate on the test set\n",
    "validation_results = evaluate_model(tokenized_datasets[\"train\"], model, tokenizer)\n",
    "print(\"Validation ROUGE Scores:\", validation_results)"
   ],
   "id": "995380d8dcd63dad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Title', 'Article', 'Summary', 'Category', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2225\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1532: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  if max_position_embeddings is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE Scores: {'rouge1': 0.10676138348564734, 'rouge2': 0.0996934416848245, 'rougeL': 0.0900784962347142}\n"
     ]
    }
   ],
   "execution_count": 149
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
