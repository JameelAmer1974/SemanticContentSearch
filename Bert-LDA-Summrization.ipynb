{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:11:50.161425Z",
     "start_time": "2025-04-06T17:11:31.883795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "id": "22a62d9338d518d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jameelamer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:11:51.955914Z",
     "start_time": "2025-04-06T17:11:50.230451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:11:53.512099Z",
     "start_time": "2025-04-06T17:11:52.046535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel"
   ],
   "id": "87028078ec5eaf3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Notes:\n",
    "- gensim is an open-source Python library used for natural language processing (NLP), particularly in topic modeling, document similarity, and word embeddings. It is designed to handle large text datasets efficiently and provides implementations of various algorithms like Latent Dirichlet Allocation (LDA), Word2Vec, FastText, and Doc2Vec.\n",
    "- sentence-transformers is a Python library for computing dense vector representations (embeddings) of sentences, paragraphs, and text documents using BERT-based models. It is widely used for tasks like semantic search, clustering, similarity measurement, and extractive summarization.\n",
    "- sklearn.metrics.pairwise is a module in scikit-learn that provides functions for computing pairwise distances and similarity scores between vectors. It is widely used for tasks like clustering, nearest neighbor search, and similarity-based ranking."
   ],
   "id": "9eb45a8e7ef80835"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:12:09.722948Z",
     "start_time": "2025-04-06T17:12:07.194325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load SBERT model\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ],
   "id": "ca60e51a9810c608",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:12:10.797990Z",
     "start_time": "2025-04-06T17:12:10.373485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load English tokenizer, POS tagger, parser, NER from spaCy\n",
    "import spacy.cli\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"Remove extra whitespace, newlines, and tabs.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def remove_noise_and_references(text):\n",
    "    \"\"\"\n",
    "    Remove references like [1], (Smith et al., 2020), and figure/table mentions.\n",
    "    You can extend the patterns as needed.\n",
    "    \"\"\"\n",
    "    # Remove square bracket citations like [1], [12]\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # Remove in-text references like (Smith et al., 2020)\n",
    "    text = re.sub(r'\\(([^)]*et al\\.,?\\s?\\d{4})\\)', '', text)\n",
    "    \n",
    "    # Remove \"Fig. 1\", \"Table 2\", etc.\n",
    "    text = re.sub(r'(Fig\\.?|Figure|Table)\\s?\\d+[a-zA-Z]?', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove licensing and copyright boilerplate\n",
    "    text = re.sub(r'©.*?(\\.|\\n)', '', text)\n",
    "    text = re.sub(r'This article is licensed.*?(\\.|\\n)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def sentence_segmentation(text):\n",
    "    \"\"\"Segment text into individual sentences using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "    step1 = normalize_whitespace(raw_text)\n",
    "    step2 = remove_noise_and_references(step1)\n",
    "    sentences = sentence_segmentation(step2)\n",
    "    return sentences\n",
    "    #return ' '.join(str(sentence) for sentence in sentences)"
   ],
   "id": "e5a66899476382fc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:12:15.746125Z",
     "start_time": "2025-04-06T17:12:15.741324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def preprocess_text(text):\n",
    "#     sentences = text.split(\". \")  # Simple sentence splitting\n",
    "#     return [sent.strip() for sent in sentences if len(sent.strip()) > 10]\n",
    "\n",
    "def get_bert_embeddings(sentences):\n",
    "    return bert_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "def apply_lda(sentences, num_topics=5):\n",
    "    # Tokenization\n",
    "    tokenized_sentences = [sent.lower().split() for sent in sentences]\n",
    "    \n",
    "    # LDA Preparation\n",
    "    dictionary = Dictionary(tokenized_sentences)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_sentences]\n",
    "    \n",
    "    # Train LDA Model\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    # Get sentence-topic probabilities\n",
    "    topic_probs = []\n",
    "    for sent in corpus:\n",
    "        topic_dist = lda.get_document_topics(sent, minimum_probability=0)\n",
    "        topic_probs.append([prob for _, prob in topic_dist])\n",
    "    \n",
    "    return np.array(topic_probs)"
   ],
   "id": "e30a5123749e7891",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:12:24.543199Z",
     "start_time": "2025-04-06T17:12:24.538967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summary(text, summary_length=3):\n",
    "    sentences = preprocess_text(text)\n",
    "    embeddings = get_bert_embeddings(sentences)\n",
    "    \n",
    "    # Compute document embedding\n",
    "    doc_embedding = np.mean(embeddings.cpu().numpy(), axis=0).reshape(1, -1)\n",
    "    \n",
    "    # Apply LDA\n",
    "    topic_probs = apply_lda(sentences)\n",
    "    \n",
    "    # Sentence Ranking based on relevance\n",
    "    similarities = cosine_similarity(embeddings.cpu().numpy(), doc_embedding).flatten()\n",
    "    \n",
    "    # Ensure topic diversity\n",
    "    selected_sentences = []\n",
    "    selected_indices = set()\n",
    "    topic_order = np.argsort(-topic_probs.sum(axis=0))  # Prioritize top topics\n",
    "    \n",
    "    for topic in topic_order:\n",
    "        topic_sent_indices = np.argsort(-topic_probs[:, topic])  # Sentences sorted by topic importance\n",
    "        for idx in topic_sent_indices:\n",
    "            if idx not in selected_indices:\n",
    "                selected_sentences.append((sentences[idx], similarities[idx]))\n",
    "                selected_indices.add(idx)\n",
    "            if len(selected_sentences) >= summary_length:\n",
    "                break\n",
    "        if len(selected_sentences) >= summary_length:\n",
    "            break\n",
    "    \n",
    "    # Sort selected sentences by relevance\n",
    "    selected_sentences.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return \" \".join([sent[0] for sent in selected_sentences])\n"
   ],
   "id": "1f1bfcba2b23d589",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:12:31.312557Z",
     "start_time": "2025-04-06T17:12:30.401479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_text = \"\"\"Y\n",
    "القوات الأوكرانية تبدأ الانسحاب من القرم,بدأت القوات الأوكرانية الانسحاب من شبه جزيرة القرم.,\"وكان الرئيس الأوكراني المؤقت، الكسندر تورتشينوف، قد أمر بسحب جميع القوات الأوكرانية من القرم. وسيطرت قوات روسية صباح الاثنين على قاعدة بحرية أوكرانية في فيودوسيا، في ثالث هجوم من نوعه خلال 48 ساعة، وذلك بحسب تصريحات مسؤولين أوكرانيين لبي بي سي . وقال المتحدث باسم وزارة الدفاع الأوكرانية فلاديسلاف سيليزنيوف إن القوات الروسية هاجمت القاعدة وألقت القبض على الجنود الأوكرانيين في قاعدة فيودوسيا وقيدت أيادي ضباطهم. ومن المتوقع أن تسيطر الأزمة الأوكرانية على قمة مجموعة الدول الصناعية السبع في لاهاي. مواضيع قد تهمك نهاية وأكد الرئيس الأمريكي باراك أوباما خلال لقاء مع نظيره الصيني شى جين بينغ على أن \"\"واشنطن وبكين يمكنهما، بالعمل سويا، تعزيز القانون الدولي واحترام سيادة الدول\"\". وتسيطر قوات روسية حاليا على معظم القواعد العسكرية الأوكرانية في القرم التي أعلنت موسكو ضمها للاتحاد الروسي بعد استفتاء أجرته السلطات المحلية هناك. قلق بالغ وقال مارك لوين، مراسل بي بي سي في القرم، إن القوات الروسية تسيطر بشكل كامل على القاعدة، ونقلت الجنود الأوكرانيين بعيدا إلى مكان مجهول. وتعد قاعدة فيودوسيا واحدة من آخر القواعد العسكرية التي بقيت تحت سيطرة كييف، لكن قوات روسية ظلت تحاصرها لبعض الوقت، حسبما أفاد مراسلنا. واقتحمت القوات الروسية قاعدتين أخريين وسيطرت عليهما يوم الجمعة. وكان مسؤولون عسكريون روس أعلنوا في وقت سابق أن العلم الروسي أصبح يرفرف على 189 وحدة ومنشأة عسكرية أوكرانية في القرم. وقال ديفيد ستيرن مراسل بي بي سي في كييف إن الأوكرانيين يتابعون هذه التطورات بقلق بالغ. وأشار إلى أن السؤال الذي يدور الآن هو ماذا سيكون رد فعل أوكرانيا والغرب وما هي الخطوة الروسية المقبلة. وحذر قائد الناتو في أوروبا يوم الأحد من أن القوات الروسية المنتشرة على الحدود الشرقية لأوكرانيا قادرة على شن عملية تمتد حتى مولدوفا. قمة الدول الصناعية الكبرى أوباما: العقوبات الغربية على موسكو ستؤثر على الاقتصاد الروسي. ويلقي ضم روسيا لمنطقة القرم بظلاله على قمة مجموعة السبع، التي كان مزمعا عقدها منذ فترة طويلة، بشأن تهديدات الأمن النووي. ومن المتوقع أن يبحث زعماء المجموعة موقفا موحدا حيال الأزمة. وأكد الرئيس الأمريكي على أن أوروبا والولايات المتحدة متفقون على دعم الحكومة الأوكرانية وشعبها، مشيرا إلى أن العقوبات التي فرضت على موسكو ستؤثر على الاقتصاد الروسي. ومن المقرر أن يلتقي وزير الخارجية الأمريكي، جون كيري، مع نظيره الروسي، سيرغي لافروف، على هامش قمة مجموعة السبع. انقطاع الكهرباء من جهة أخرى، شكا سكان محليون في بعض مناطق القرم من انقطاع الكهرباء في وقت متأخر من الأحد. ولف الظلام العديد من المدن من بينها بعض أحياء العاصمة سيمفربول. وقالت شركة توريد الكهرباء في القرم \"\"كريمنيرغو\"\" في بيان بث على موقعها الإلكتروني إن عطلا فنيا أصاب أحد الخطوط التي تديرها شركة الكهرباء الوطنية الأوكرانية \"\"اوكرينيرغو\"\". ولم يتسن الحصول على تعليق من اوكرينيرغو، ولم يصدر أيضا تأكيد مستقل حول سبب انقطاع الكهرباء. ضم القرم وضمت روسيا القرم إليها عقب استفتاء أجري في المنطقة في 16 مارس/آذار. وجاءت الخطوة الروسية بعد أن أطاحت احتجاجات بالرئيس الأوكراني السابق الموالي لروسيا فيكتور يانوكوفيتش. وأكدت روسيا أنها تحركت لحماية مواطني القرم المتحدرين من أصول روسية ضد من وصفتهم \"\"بالفاشيين\"\" الذين انتقلوا إليها من البلد الأم أوكرانيا. وردت الولايات المتحدة والاتحاد الأوروبي بفرض سلسلة من العقوبات ضد أفراد من بينهم مسؤولون بارزون اتهمتهم واشنطن وبروكسل بلعب دور في ضم القرم. موالون لروسيا يتظاهرون في مدينة مدينة دونيتسك، شرقي أوكرانيا. تسيطر قوات روسية حاليا على معظم القواعد العسكرية الأوكرانية في القرم.\n",
    "\"\"\"\n",
    "summary = generate_summary(document_text, summary_length=3)\n",
    "print(summary)"
   ],
   "id": "7b6e542e32e00439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قلق بالغ وقال مارك لوين، مراسل بي بي سي في القرم، إن القوات الروسية تسيطر بشكل كامل على القاعدة، ونقلت الجنود الأوكرانيين بعيدا إلى مكان مجهول. وأكدت روسيا أنها تحركت لحماية مواطني القرم المتحدرين من أصول روسية ضد من وصفتهم \"\"بالفاشيين\"\" الذين انتقلوا إليها من البلد الأم أوكرانيا. وتسيطر قوات روسية حاليا على معظم القواعد العسكرية الأوكرانية في القرم التي أعلنت موسكو ضمها للاتحاد الروسي بعد استفتاء أجرته السلطات المحلية هناك.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "\n",
    "\n",
    "Evaluation For BBC_news_dataset"
   ],
   "id": "a8bc7f2013e547c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:12:38.671570Z",
     "start_time": "2025-04-06T17:12:38.589377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Load BBC dataset\n",
    "df = pd.read_csv('bbc_news_with_articles_and_extractive_summary.csv')\n",
    "\n",
    "print(df.head())"
   ],
   "id": "dfbcceadd29a6501",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Title                                            Article  \\\n",
      "0           0    289  Musicians to tackle US red tape\\n\\nMusicians' ...   \n",
      "1           1    262  U2's desire to be number one\\n\\nU2, who have w...   \n",
      "2           2    276  Rocker Doherty in on-stage fight\\n\\nRock singe...   \n",
      "3           3     60  Snicket tops US box office chart\\n\\nThe film a...   \n",
      "4           4     74  Ocean's Twelve raids box office\\n\\nOcean's Twe...   \n",
      "\n",
      "                                             Summary       Category  \\\n",
      "0  Nigel McCune from the Musicians' Union said Br...  entertainment   \n",
      "1  But they still want more.They have to want to ...  entertainment   \n",
      "2  Babyshambles, which he formed after his acrimo...  entertainment   \n",
      "3  A Series of Unfortunate Events also stars Scot...  entertainment   \n",
      "4  Ocean's Twelve, the crime caper sequel starrin...  entertainment   \n",
      "\n",
      "                                  extractive_summary  \n",
      "0  Nigel McCune from the Musicians' Union said Br...  \n",
      "1  They have to want to be the biggest band ever ...  \n",
      "2  Babyshambles, which he formed after his acrimo...  \n",
      "3  A Series of Unfortunate Events, starring Jim C...  \n",
      "4  Ocean's Twelve raids box office\\n\\nOcean's Twe...  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:13:49.802542Z",
     "start_time": "2025-04-06T17:13:49.794767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remove stopwords and punctuation\n",
    "    return tokens\n",
    "\n"
   ],
   "id": "5d7565475ca10e68",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jameelamer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jameelamer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:13:56.719442Z",
     "start_time": "2025-04-06T17:13:53.903388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply preprocessing\n",
    "df[\"processed_Article\"] = df[\"Article\"].apply(preprocess)\n",
    "df[\"processed_summary\"] = df[\"Summary\"].apply(preprocess)\n"
   ],
   "id": "987d4d0284290272",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:14:06.182187Z",
     "start_time": "2025-04-06T17:13:58.940592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "dictionary = Dictionary(df[\"processed_Article\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in df[\"processed_Article\"]]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ],
   "id": "7b377a286037b620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.017*\"said\" + 0.007*\"people\" + 0.007*\"software\" + 0.005*\"search\" + 0.005*\"would\"')\n",
      "(1, '0.023*\"said\" + 0.013*\"mr\" + 0.008*\"would\" + 0.007*\"people\" + 0.006*\"government\"')\n",
      "(2, '0.010*\"game\" + 0.007*\"said\" + 0.005*\"like\" + 0.005*\"time\" + 0.004*\"new\"')\n",
      "(3, '0.013*\"said\" + 0.009*\"mobile\" + 0.009*\"people\" + 0.007*\"technology\" + 0.007*\"music\"')\n",
      "(4, '0.010*\"said\" + 0.006*\"best\" + 0.005*\"year\" + 0.005*\"film\" + 0.005*\"also\"')\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:17:59.582212Z",
     "start_time": "2025-04-06T17:14:25.143796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "#bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_summary(text, lda_model, dictionary, bert_model, num_sentences=3):\n",
    "    # Split into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text  # If too short, return the full text\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    sentence_embeddings = bert_model.encode(sentences)\n",
    "\n",
    "    # Compute topic distribution\n",
    "    bow_vector = dictionary.doc2bow(preprocess(text))\n",
    "    topic_distribution = lda_model.get_document_topics(bow_vector)\n",
    "\n",
    "    # Get dominant topic\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Get topic words\n",
    "    topic_words = [word for word, _ in lda_model.show_topic(dominant_topic)]\n",
    "\n",
    "    # Compute similarity to topic words\n",
    "    topic_embedding = bert_model.encode(\" \".join(topic_words))\n",
    "    sentence_scores = [cosine_similarity([sent_emb], [topic_embedding])[0][0] for sent_emb in sentence_embeddings]\n",
    "\n",
    "    # Select top sentences\n",
    "    summary_sentences = [sentences[i] for i in np.argsort(sentence_scores)[-num_sentences:]]\n",
    "    return \" \".join(summary_sentences)\n",
    "\n",
    "# Generate summaries\n",
    "df[\"generated_summary\"] = df[\"Article\"].apply(lambda x: extract_summary(x, lda_model, dictionary, bert_model))\n"
   ],
   "id": "764cb22e749e1bf4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:18:08.676266Z",
     "start_time": "2025-04-06T17:17:59.638834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def compute_rouge(reference, generated):\n",
    "    scores = rouge.get_scores(generated, reference)\n",
    "    return scores[0]  # Returns ROUGE-1, ROUGE-2, ROUGE-L scores\n",
    "\n",
    "# Evaluate summaries\n",
    "df[\"rouge_scores\"] = df.apply(lambda row: compute_rouge(row[\"Summary\"], row[\"generated_summary\"]), axis=1)\n",
    "\n",
    "# Display average ROUGE scores\n",
    "rouge_l_scores = [score[\"rouge-l\"][\"f\"] for score in df[\"rouge_scores\"]]\n",
    "print(f\"Average ROUGE-L Score: {sum(rouge_l_scores) / len(rouge_l_scores):.4f}\")\n"
   ],
   "id": "59fe1ed040380fc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-L Score: 0.3871\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:38:08.219791Z",
     "start_time": "2025-04-06T17:38:08.193775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display average ROUGE scores\n",
    "rouge_l_scores = [score[\"rouge-2\"][\"f\"] for score in df[\"rouge_scores\"]]\n",
    "print(f\"Average ROUGE-2 Score: {sum(rouge_l_scores) / len(rouge_l_scores):.4f}\")"
   ],
   "id": "28008402a6f130de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-2 Score: 0.2725\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:38:43.878606Z",
     "start_time": "2025-04-06T17:38:43.857836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display average ROUGE scores\n",
    "rouge_l_scores = [score[\"rouge-1\"][\"f\"] for score in df[\"rouge_scores\"]]\n",
    "print(f\"Average ROUGE-1 Score: {sum(rouge_l_scores) / len(rouge_l_scores):.4f}\")"
   ],
   "id": "b98e773648d5abda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 Score: 0.3988\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:31:29.890410Z",
     "start_time": "2025-04-06T17:29:43.109537Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.6558\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "def compute_similarity(reference, generated):\n",
    "    ref_embedding = bert_model.encode(reference)\n",
    "    gen_embedding = bert_model.encode(generated)\n",
    "    return cosine_similarity([ref_embedding], [gen_embedding])[0][0]\n",
    "\n",
    "df[\"cosine_similarity\"] = df.apply(lambda row: compute_similarity(row[\"Summary\"], row[\"generated_summary\"]), axis=1)\n",
    "print(f\"Average Cosine Similarity: {df['cosine_similarity'].mean():.4f}\")\n"
   ],
   "id": "3ea633043507d490"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:35:34.311328Z",
     "start_time": "2025-03-21T10:35:28.294738Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence Score: 0.3597\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df[\"processed_Article\"], dictionary=dictionary, coherence=\"c_v\")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Topic Coherence Score: {coherence_score:.4f}\")\n"
   ],
   "id": "4ee9580a52deb38e"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "\n",
    "\n",
    "# Finetune Bert model \n",
    "\n"
   ],
   "id": "e59d969d6642bd18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:36:05.708208Z",
     "start_time": "2025-03-21T10:36:05.621713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset \n",
    "BBC_df = pd.read_csv(\"bbc_news_summary_with_articles.csv\")\n",
    "BBC_df.dropna(subset=['Article', 'Summary'], inplace=True)\n",
    "# Split into 80% train and 20% test\n",
    "train_df, test_df = train_test_split(BBC_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show dataset sizes\n",
    "print(f\"Train Size: {len(train_df)}, Test Size: {len(test_df)}\")\n"
   ],
   "id": "9c15548cfab78324",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 1780, Test Size: 445\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:36:08.363234Z",
     "start_time": "2025-03-21T10:36:08.331462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "# Create InputExamples for training\n",
    "train_examples = []\n",
    "for i, row in train_df.iterrows():\n",
    "    train_examples.append(InputExample(\n",
    "        texts=[row['Article'], row['Summary']],\n",
    "        label=1.0  # Label of 1.0 for positive (similar) pairs\n",
    "    ))\n",
    "\n"
   ],
   "id": "8897c767c6fd7eb0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:36:10.803933Z",
     "start_time": "2025-03-21T10:36:10.801467Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_df.columns)",
   "id": "bb41cf8d1f1326a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Title', 'Article', 'Summary', 'Category'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:53:12.165487Z",
     "start_time": "2025-03-21T09:53:08.560945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, losses, InputExample,models,util\n",
    "from torch.utils.data import DataLoader\n",
    "# Convert the examples into a DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Use CosineSimilarityLoss for fine-tuning the model\n",
    "train_loss = losses.CosineSimilarityLoss(bert_model)"
   ],
   "id": "d291f95bc5848faf",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:58:02.503239Z",
     "start_time": "2025-03-21T09:54:27.675297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "bert_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=4,  # Set this to more if needed\n",
    "    warmup_steps=100,\n",
    "    output_path='./sbert_finetuned/sbert_bbc_finetuned'\n",
    ")"
   ],
   "id": "b7c3ba927463d079",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='448' max='448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [448/448 03:32, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed82776546496175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:36:26.059535Z",
     "start_time": "2025-03-21T10:36:25.028923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "# Apply preprocessing\n",
    "test_df[\"processed_Article\"] = test_df[\"Article\"].apply(preprocess)\n",
    "test_df[\"processed_summary\"] = test_df[\"Summary\"].apply(preprocess)"
   ],
   "id": "fd7e1c05d90c9ef7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:36:35.330309Z",
     "start_time": "2025-03-21T10:36:32.610037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "dictionary = Dictionary(test_df[\"processed_Article\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in test_df[\"processed_Article\"]]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ],
   "id": "3a35b61a84736ed9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.020*\"said\" + 0.010*\"mr\" + 0.007*\"would\" + 0.005*\"labour\" + 0.004*\"government\"')\n",
      "(1, '0.009*\"said\" + 0.007*\"best\" + 0.006*\"music\" + 0.005*\"one\" + 0.005*\"years\"')\n",
      "(2, '0.012*\"said\" + 0.008*\"would\" + 0.006*\"mr\" + 0.005*\"new\" + 0.004*\"also\"')\n",
      "(3, '0.016*\"said\" + 0.004*\"us\" + 0.004*\"also\" + 0.003*\"would\" + 0.003*\"mr\"')\n",
      "(4, '0.009*\"said\" + 0.006*\"us\" + 0.004*\"film\" + 0.004*\"software\" + 0.004*\"new\"')\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:36:55.272337Z",
     "start_time": "2025-03-21T10:36:38.826888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate summaries\n",
    "test_df[\"generated_summary\"] = test_df[\"Article\"].apply(lambda x: extract_summary(x, lda_model, dictionary, bert_model))\n"
   ],
   "id": "8bd73da900616e42",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:37:00.603468Z",
     "start_time": "2025-03-21T10:36:58.336047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Evaluate summaries\n",
    "test_df[\"rouge_scores\"] = test_df.apply(lambda row: compute_rouge(row[\"Summary\"], row[\"generated_summary\"]), axis=1)\n",
    "\n",
    "# Display average ROUGE scores\n",
    "rouge_l_scores = [score[\"rouge-l\"][\"f\"] for score in df[\"rouge_scores\"]]\n",
    "print(f\"Average ROUGE-L Score after finetune: {sum(rouge_l_scores) / len(rouge_l_scores):.4f}\")"
   ],
   "id": "60386a430fa15fd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-L Score after finetune: 0.3630\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:37:13.893357Z",
     "start_time": "2025-03-21T10:37:03.710664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df[\"cosine_similarity\"] = test_df.apply(lambda row: compute_similarity(row[\"Summary\"], row[\"generated_summary\"]), axis=1)\n",
    "print(f\"Average Cosine Similarity: {df['cosine_similarity'].mean():.4f}\")"
   ],
   "id": "450709d810817594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.6261\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ed1ce1ede523657"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
