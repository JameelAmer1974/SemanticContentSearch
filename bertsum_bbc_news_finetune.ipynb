{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:41:55.899068Z",
     "start_time": "2025-04-07T11:41:51.729038Z"
    }
   },
   "source": [
    "import nltk\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "# Load dataset (replace with your actual dataset loading code)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "dataset = load_from_disk(\"bbc_dataset\") \n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jameelamer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:42:01.715872Z",
     "start_time": "2025-04-07T11:42:01.464956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Constants\n",
    "MAX_SENTENCES = 8\n",
    "SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 4"
   ],
   "id": "796c288b0762a9a8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fae4d2665e0e63a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:42:04.745491Z",
     "start_time": "2025-04-07T11:42:02.891096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Preprocessing Function\n",
    "def preprocess_function(examples):\n",
    "    tokenized_articles = []\n",
    "    tokenized_masks = []\n",
    "    labels_list = []\n",
    "\n",
    "    for article, summary in zip(examples[\"Article\"], examples[\"extractive_summary\"]):\n",
    "        # Clean inputs\n",
    "        article = \" \".join(article) if isinstance(article, list) else article\n",
    "        summary = \" \".join(summary) if isinstance(summary, list) else summary\n",
    "        if (summary==\"\"):\n",
    "            return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []} \n",
    "        # Tokenize sentences\n",
    "        sentences = nltk.sent_tokenize(article)[:MAX_SENTENCES]\n",
    "        num_sentences = len(sentences)\n",
    "        \n",
    "        # Tokenize all sentences\n",
    "        tokenized = tokenizer(\n",
    "            sentences,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=SEQ_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create labels (1 if sentence is in summary)\n",
    "        labels = [1 if sent in summary else 0 for sent in sentences]\n",
    "        labels += [0] * (MAX_SENTENCES - num_sentences)  # Pad labels\n",
    "        \n",
    "        # Pad tensors\n",
    "        padded_input_ids = torch.zeros((MAX_SENTENCES, SEQ_LENGTH), dtype=torch.long)\n",
    "        padded_attention_mask = torch.zeros((MAX_SENTENCES, SEQ_LENGTH), dtype=torch.long)\n",
    "        \n",
    "        padded_input_ids[:num_sentences] = tokenized[\"input_ids\"]\n",
    "        padded_attention_mask[:num_sentences] = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        tokenized_articles.append(padded_input_ids)\n",
    "        tokenized_masks.append(padded_attention_mask)\n",
    "        labels_list.append(torch.tensor(labels, dtype=torch.float))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_articles,\n",
    "        \"attention_mask\": tokenized_masks,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "# 2. Apply Preprocessing\n",
    "\n",
    "val_dataset = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n"
   ],
   "id": "3e7e3dac946e138e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 445/445 [00:00<00:00, 797.20 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1335/1335 [00:01<00:00, 1044.37 examples/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:42:08.308867Z",
     "start_time": "2025-04-07T11:42:08.304909Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset.column_names",
   "id": "8f5a1d996461bc9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title',\n",
       " 'Article',\n",
       " 'Summary',\n",
       " 'Category',\n",
       " 'extractive_summary',\n",
       " 'input_ids',\n",
       " 'attention_mask',\n",
       " 'labels']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:42:11.415696Z",
     "start_time": "2025-04-07T11:42:11.405975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove unnecessary columns\n",
    "columns_to_remove = [\"filename\", \"Article\", \"Summary\", \"__index_level_0__\",\"extractive_summary\"]\n",
    "train_dataset = train_dataset.remove_columns([col for col in columns_to_remove if col in train_dataset.column_names])\n",
    "val_dataset = val_dataset.remove_columns([col for col in columns_to_remove if col in val_dataset.column_names])\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ],
   "id": "a225b572368b66d2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:42:14.608943Z",
     "start_time": "2025-04-07T11:42:14.605377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom Collate Function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    "\n",
    "\n",
    "# Custom Trainer Class\n",
    "class SentenceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Add **kwargs\n",
    "        # Reshape inputs: [batch, sentences, seq_len] -> [batch*sentences, seq_len]\n",
    "        batch_size, num_sentences, seq_len = inputs[\"input_ids\"].shape\n",
    "        flat_inputs = {\n",
    "            \"input_ids\": inputs[\"input_ids\"].view(-1, seq_len),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].view(-1, seq_len)\n",
    "        }\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**flat_inputs)\n",
    "        logits = outputs.logits.view(batch_size, num_sentences)\n",
    "        \n",
    "        # BCEWithLogitsLoss for multi-label classification\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, inputs[\"labels\"])\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ],
   "id": "e2e4834cc19ce47c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:42:19.015644Z",
     "start_time": "2025-04-07T11:42:18.594899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=1,  # Binary classification per sentence\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n"
   ],
   "id": "248456b81f49842f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:42:21.445304Z",
     "start_time": "2025-04-07T11:42:21.407110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=15000,\n",
    "    save_steps=30000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    ")"
   ],
   "id": "73d2b4ea6799231a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameelamer/MasterProject/Summarization/summrization_webapp/flaskProject2/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:56:54.542157Z",
     "start_time": "2025-04-07T11:42:26.471165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Trainer\n",
    "# Initialize Trainer with the fixed class\n",
    "trainer = SentenceTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# 3. Start training\n",
    "trainer.train()\n",
    "\n",
    "#  Save the final model\n",
    "trainer.save_model(\"./bertsum_bbc_news/bertsum_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./bertsum_bbc_news/bertsum_finetuned_model\")"
   ],
   "id": "103646af15180f1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 14:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./bertsum_bbc_news/bertsum_finetuned_model/tokenizer_config.json',\n",
       " './bertsum_bbc_news/bertsum_finetuned_model/special_tokens_map.json',\n",
       " './bertsum_bbc_news/bertsum_finetuned_model/vocab.txt',\n",
       " './bertsum_bbc_news/bertsum_finetuned_model/added_tokens.json',\n",
       " './bertsum_bbc_news/bertsum_finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:03:14.644451Z",
     "start_time": "2025-04-07T12:03:14.632112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def generate_summary(model, tokenizer, text, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Move inputs to the correct device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)  # Forward pass on MPS\n",
    "        logits = output.logits.squeeze(-1)  # Extract logits\n",
    "        \n",
    "        # Ensure logits are moved to CPU before processing\n",
    "        logits = logits.cpu()\n",
    "\n",
    "        # Select sentences using thresholding\n",
    "        predicted_labels = (logits > 0.5).int()\n",
    "        \n",
    "        sentences = text.split(\". \")  # Sentence tokenization\n",
    "        min_length = min(len(sentences), len(predicted_labels))\n",
    "        # print(\"Logits:\", logits)\n",
    "        # print(\"Predicted Labels:\", predicted_labels)\n",
    "        # print(\"Sentences:\", sentences)\n",
    "        selected_sentences = [sentences[i] for i in range(min_length) if predicted_labels[i] == 1]        \n",
    "        summary = \" \".join(selected_sentences)\n",
    "        return summary\n",
    "\n",
    "# Detect MPS device on Mac\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to MPS\n"
   ],
   "id": "7fac6f9cc08a8374",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:03:20.235533Z",
     "start_time": "2025-04-07T12:03:18.554428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example Usage:\n",
    "text = \"\"\"\n",
    "Jamieson issues warning to bigots\\n\\nScotland's justice minister has warned bigoted soccer fans that she wants to hit them \"where it hurts most\" by banning them from matches.\\n\\nCathy Jamieson said exclusion orders are one of a series of measures being considered in the Scottish Executive campaign against sectarianism. She praised Celtic and Rangers for their work in tackling the problem. However, the minister said stopping sectarian abuse associated with Old Firm matches is a key objective. Ms Jamieson was speaking ahead of the third round Scottish Cup clash between the Glasgow clubs at Parkhead on Sunday. The sectarianism long associated with sections of the support from both clubs has become a significant target for the executive. Last week Ms Jamieson and First Minister Jack McConnell met supporters' representatives from both clubs to discuss the issue.\\n\\nThey plan to hold an anti-sectarian summit next month with officials from the clubs, church leaders, senior police officers and local authority chiefs among those to be invited. Speaking on BBC Radio Scotland's Sunday Live programme, Ms Jamieson described Friday's meeting as \"very productive\" and said putting the squeeze on the bigots would be a key aim. Ms Jamieson stressed that sectarianism has not been confined to football but it can act as a \"trigger\" for tensions and violence. Clubs have taken action in the past to ban troublesome fans and supporters' groups expressed their desire to ensure that the game is no longer tainted by the problem.\\n\\nMs Jamieson said the executive should have a role in tackling the soccer troublemakers. She said: \"We can't get away from the fact that in some instances some of the religious hatred that some people try to associate with football boils over into violence. \"That is the kind of thing we want to stop and that's the kind of thing supporters' groups are very clear they don't want to be part of either, and they will work with us to try and deal with that.\"\\n\\nMs Jamieson praised the police for their action and said: \"The police do want to identify whether there are particular individuals who are going over the top and inciting hatred or violence - they will crack down very effectively on them. \"We have of course already indicated that we will consider the introduction of banning orders to give additional powers to where there are people who are going over the top, who have made inappropriate behaviour at football matches, to be able to stop them attending the games. \"That's the kind of thing that will hit those kind of people where it hurts the most in not allowing them to attend the games,\" she said. Praising Celtic and Rangers for their efforts, she said: \"I don't think there is any doubt that we have seen some positive moves from the clubs. \"Both Rangers and Celtic football clubs have been involved in working with the executive to produce, for example, an educational pack for  \"\"\"\n",
    "summary = generate_summary(model, tokenizer, text, device)\n",
    "print(\"Generated Summary:\", summary)\n"
   ],
   "id": "7c3df997b965ad73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: \n",
      "Jamieson issues warning to bigots\n",
      "\n",
      "Scotland's justice minister has warned bigoted soccer fans that she wants to hit them \"where it hurts most\" by banning them from matches.\n",
      "\n",
      "Cathy Jamieson said exclusion orders are one of a series of measures being considered in the Scottish Executive campaign against sectarianism\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cbb3d32c96ef705"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "df1482bfd1d34860"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
