{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T14:08:47.357384Z",
     "start_time": "2025-03-29T14:08:47.346663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "# Load dataset (replace with your actual dataset loading code)\n",
    "dataset = load_from_disk(\"bbc_dataset\") \n"
   ],
   "id": "798f8886354804c3",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T14:50:28.713476Z",
     "start_time": "2025-03-29T14:30:53.351300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Constants\n",
    "MAX_SENTENCES = 8\n",
    "SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# 1. Preprocessing Function\n",
    "def preprocess_function(examples):\n",
    "    tokenized_articles = []\n",
    "    tokenized_masks = []\n",
    "    labels_list = []\n",
    "\n",
    "    for article, summary in zip(examples[\"Article\"], examples[\"extractive_summary\"]):\n",
    "        # Clean inputs\n",
    "        article = \" \".join(article) if isinstance(article, list) else article\n",
    "        summary = \" \".join(summary) if isinstance(summary, list) else summary\n",
    "        \n",
    "        # Tokenize sentences\n",
    "        sentences = nltk.sent_tokenize(article)[:MAX_SENTENCES]\n",
    "        num_sentences = len(sentences)\n",
    "        \n",
    "        # Tokenize all sentences\n",
    "        tokenized = tokenizer(\n",
    "            sentences,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=SEQ_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create labels (1 if sentence is in summary)\n",
    "        labels = [1 if sent in summary else 0 for sent in sentences]\n",
    "        labels += [0] * (MAX_SENTENCES - num_sentences)  # Pad labels\n",
    "        \n",
    "        # Pad tensors\n",
    "        padded_input_ids = torch.zeros((MAX_SENTENCES, SEQ_LENGTH), dtype=torch.long)\n",
    "        padded_attention_mask = torch.zeros((MAX_SENTENCES, SEQ_LENGTH), dtype=torch.long)\n",
    "        \n",
    "        padded_input_ids[:num_sentences] = tokenized[\"input_ids\"]\n",
    "        padded_attention_mask[:num_sentences] = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        tokenized_articles.append(padded_input_ids)\n",
    "        tokenized_masks.append(padded_attention_mask)\n",
    "        labels_list.append(torch.tensor(labels, dtype=torch.float))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_articles,\n",
    "        \"attention_mask\": tokenized_masks,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "# 2. Apply Preprocessing\n",
    "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "val_dataset = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = [\"Title\", \"Article\", \"Summary\", \"Category\", \"extractive_summary\"]\n",
    "train_dataset = train_dataset.remove_columns([col for col in columns_to_remove if col in train_dataset.column_names])\n",
    "val_dataset = val_dataset.remove_columns([col for col in columns_to_remove if col in val_dataset.column_names])\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 3. Custom Collate Function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    "\n",
    "# 4. Initialize Model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=1,  # Binary classification per sentence\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# 5. Custom Trainer Class\n",
    "class SentenceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Add **kwargs\n",
    "        # Reshape inputs: [batch, sentences, seq_len] -> [batch*sentences, seq_len]\n",
    "        batch_size, num_sentences, seq_len = inputs[\"input_ids\"].shape\n",
    "        flat_inputs = {\n",
    "            \"input_ids\": inputs[\"input_ids\"].view(-1, seq_len),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].view(-1, seq_len)\n",
    "        }\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**flat_inputs)\n",
    "        logits = outputs.logits.view(batch_size, num_sentences)\n",
    "        \n",
    "        # BCEWithLogitsLoss for multi-label classification\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, inputs[\"labels\"])\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 6. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    ")\n",
    "\n",
    "# 7. Create Trainer\n",
    "# 2. Initialize Trainer with the fixed class\n",
    "trainer = SentenceTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# 3. Start training\n",
    "trainer.train()\n",
    "\n",
    "# 9. Save the final model\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n"
   ],
   "id": "1da561c04bbde3a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jameelamer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Map: 100%|██████████| 222/222 [00:00<00:00, 1077.55 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1335' max='1335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1335/1335 19:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.349781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.339605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/vocab.txt',\n",
       " './final_model/added_tokens.json',\n",
       " './final_model/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "609bf0558c5ac1f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:04:47.559811Z",
     "start_time": "2025-03-29T16:04:45.840899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def generate_summary(model, tokenizer, text, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Move inputs to the correct device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)  # Forward pass on MPS\n",
    "        logits = output.logits.squeeze(-1)  # Extract logits\n",
    "        \n",
    "        # Ensure logits are moved to CPU before processing\n",
    "        logits = logits.cpu()\n",
    "\n",
    "        # Select sentences using thresholding\n",
    "        predicted_labels = (logits > 0.5).int()\n",
    "        \n",
    "        sentences = text.split(\". \")  # Sentence tokenization\n",
    "        min_length = min(len(sentences), len(predicted_labels))\n",
    "        # print(\"Logits:\", logits)\n",
    "        # print(\"Predicted Labels:\", predicted_labels)\n",
    "        # print(\"Sentences:\", sentences)\n",
    "        selected_sentences = [sentences[i] for i in range(min_length) if predicted_labels[i] == 1]        \n",
    "        summary = \" \".join(selected_sentences)\n",
    "        return summary\n",
    "\n",
    "# Detect MPS device on Mac\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to MPS\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "text = \"\"\"\n",
    "Jamieson issues warning to bigots\\n\\nScotland's justice minister has warned bigoted soccer fans that she wants to hit them \"where it hurts most\" by banning them from matches.\\n\\nCathy Jamieson said exclusion orders are one of a series of measures being considered in the Scottish Executive campaign against sectarianism. She praised Celtic and Rangers for their work in tackling the problem. However, the minister said stopping sectarian abuse associated with Old Firm matches is a key objective. Ms Jamieson was speaking ahead of the third round Scottish Cup clash between the Glasgow clubs at Parkhead on Sunday. The sectarianism long associated with sections of the support from both clubs has become a significant target for the executive. Last week Ms Jamieson and First Minister Jack McConnell met supporters' representatives from both clubs to discuss the issue.\\n\\nThey plan to hold an anti-sectarian summit next month with officials from the clubs, church leaders, senior police officers and local authority chiefs among those to be invited. Speaking on BBC Radio Scotland's Sunday Live programme, Ms Jamieson described Friday's meeting as \"very productive\" and said putting the squeeze on the bigots would be a key aim. Ms Jamieson stressed that sectarianism has not been confined to football but it can act as a \"trigger\" for tensions and violence. Clubs have taken action in the past to ban troublesome fans and supporters' groups expressed their desire to ensure that the game is no longer tainted by the problem.\\n\\nMs Jamieson said the executive should have a role in tackling the soccer troublemakers. She said: \"We can't get away from the fact that in some instances some of the religious hatred that some people try to associate with football boils over into violence. \"That is the kind of thing we want to stop and that's the kind of thing supporters' groups are very clear they don't want to be part of either, and they will work with us to try and deal with that.\"\\n\\nMs Jamieson praised the police for their action and said: \"The police do want to identify whether there are particular individuals who are going over the top and inciting hatred or violence - they will crack down very effectively on them. \"We have of course already indicated that we will consider the introduction of banning orders to give additional powers to where there are people who are going over the top, who have made inappropriate behaviour at football matches, to be able to stop them attending the games. \"That's the kind of thing that will hit those kind of people where it hurts the most in not allowing them to attend the games,\" she said. Praising Celtic and Rangers for their efforts, she said: \"I don't think there is any doubt that we have seen some positive moves from the clubs. \"Both Rangers and Celtic football clubs have been involved in working with the executive to produce, for example, an educational pack for  \"\"\"\n",
    "summary = generate_summary(model, tokenizer, text, device)\n",
    "print(\"Generated Summary:\", summary)\n",
    "\n"
   ],
   "id": "591f8ae4627e640f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: \n",
      "Jamieson issues warning to bigots\n",
      "\n",
      "Scotland's justice minister has warned bigoted soccer fans that she wants to hit them \"where it hurts most\" by banning them from matches.\n",
      "\n",
      "Cathy Jamieson said exclusion orders are one of a series of measures being considered in the Scottish Executive campaign against sectarianism\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c9a86787d372512f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7de29612a4559e3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T14:52:22.988071Z",
     "start_time": "2025-03-29T14:52:08.444824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results (loss, accuracy, etc.)\n",
    "print(\"Evaluation Results:\", results)"
   ],
   "id": "98a70ca804623cbc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.33960482478141785, 'eval_runtime': 14.5197, 'eval_samples_per_second': 15.29, 'eval_steps_per_second': 3.857, 'epoch': 3.0}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:02:44.954833Z",
     "start_time": "2025-03-29T15:02:30.006139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run evaluation on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Keys in eval_results:\n",
    "# - eval_loss: Average loss across validation set\n",
    "# - eval_runtime: Time taken\n",
    "# - eval_samples_per_second: Throughput\n",
    "# - eval_steps_per_second"
   ],
   "id": "14a2b38622e63f0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:13]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.33960482478141785, 'eval_runtime': 14.9391, 'eval_samples_per_second': 14.86, 'eval_steps_per_second': 3.749, 'epoch': 3.0}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:03:11.276813Z",
     "start_time": "2025-03-29T15:03:11.243584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert logits to binary predictions (0 or 1)\n",
    "    preds = (torch.sigmoid(torch.tensor(predictions)) > 0.5).int().numpy()\n",
    "    \n",
    "    # Flatten all sentences across all articles\n",
    "    flat_preds = preds.ravel()\n",
    "    flat_labels = labels.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        flat_labels, flat_preds, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(flat_labels, flat_preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Update trainer initialization:\n",
    "trainer = SentenceTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,  # Add this line\n",
    ")"
   ],
   "id": "af1c13eeddf9a9ed",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:46:17.107952Z",
     "start_time": "2025-03-29T15:46:17.090454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(dataset, trainer, tokenizer, max_sentences=8):\n",
    "    \"\"\"\n",
    "    Fixed evaluation function that handles:\n",
    "    - Sample alignment\n",
    "    - Tensor/list conversions\n",
    "    - Metric calculations\n",
    "    \"\"\"\n",
    "    # 1. Preprocess dataset to match model inputs\n",
    "    processed_dataset = dataset.map(\n",
    "        lambda x: preprocess_function({'Article': [x['Article']], \n",
    "                                     'extractive_summary': [x['extractive_summary']]}),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # 2. Convert to torch tensors\n",
    "    processed_dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=['input_ids', 'attention_mask', 'labels']\n",
    "    )\n",
    "    \n",
    "    # 3. Get predictions in batches\n",
    "    try:\n",
    "        predictions = trainer.predict(processed_dataset)\n",
    "        pred_logits = predictions.predictions\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 4. Convert to binary predictions\n",
    "    pred_probs = torch.sigmoid(torch.tensor(pred_logits))\n",
    "    final_preds = (pred_probs > 0.5).int().numpy()\n",
    "    \n",
    "    # 5. Get original texts\n",
    "    articles = dataset[\"Article\"]\n",
    "    true_summaries = dataset[\"extractive_summary\"]\n",
    "    \n",
    "    # 6. Verify alignment\n",
    "    if len(articles) != len(final_preds):\n",
    "        print(f\"Alignment warning: {len(articles)} articles vs {len(final_preds)} predictions\")\n",
    "        min_len = min(len(articles), len(final_preds))\n",
    "        articles = articles[:min_len]\n",
    "        true_summaries = true_summaries[:min_len]\n",
    "        final_preds = final_preds[:min_len]\n",
    "    \n",
    "    # 7. Calculate metrics\n",
    "    results = {\n",
    "        'correct': 0,\n",
    "        'total_predicted': 0,\n",
    "        'total_actual': 0,\n",
    "        'total_sentences': 0\n",
    "    }\n",
    "    \n",
    "    for i, (article, true_summary) in enumerate(zip(articles, true_summaries)):\n",
    "        sentences = nltk.sent_tokenize(str(article))[:max_sentences]\n",
    "        true_labels = [1 if str(sent) in str(true_summary) else 0 for sent in sentences]\n",
    "        true_labels += [0] * (max_sentences - len(true_labels))\n",
    "        \n",
    "        # Handle prediction alignment\n",
    "        article_preds = final_preds[i][:len(true_labels)]\n",
    "        \n",
    "        for true, pred in zip(true_labels, article_preds):\n",
    "            results['total_sentences'] += 1\n",
    "            if pred == 1:\n",
    "                results['total_predicted'] += 1\n",
    "                if true == 1:\n",
    "                    results['correct'] += 1\n",
    "            if true == 1:\n",
    "                results['total_actual'] += 1\n",
    "    \n",
    "    # 8. Calculate final metrics\n",
    "    eps = 1e-9\n",
    "    metrics = {\n",
    "        'precision': results['correct'] / (results['total_predicted'] + eps),\n",
    "        'recall': results['correct'] / (results['total_actual'] + eps),\n",
    "        'f1': 2 * (precision * recall) / (precision + recall + eps),\n",
    "        'accuracy': results['correct'] / (results['total_sentences'] + eps),\n",
    "        'num_samples': len(articles)\n",
    "    }\n",
    "    \n",
    "    # 9. Calculate ROUGE scores if possible\n",
    "    try:\n",
    "        pred_summaries = generate_summaries(articles, final_preds)\n",
    "        rouge_scores = calculate_rouge(pred_summaries, true_summaries)\n",
    "        metrics.update({\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"ROUGE calculation failed: {e}\")\n",
    "    \n",
    "    return metrics"
   ],
   "id": "4131c306e211da61",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:46:27.006483Z",
     "start_time": "2025-03-29T15:46:25.691616Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_model(dataset[\"validation\"], trainer, tokenizer)",
   "id": "26196b1299927245",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 222/222 [00:00<00:00, 4184.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction failed: Found input variables with inconsistent numbers of samples: [8, 1]\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:25:12.404214Z",
     "start_time": "2025-03-29T15:25:12.395885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_sentence_selection(articles, true_summaries, preds, tokenizer):\n",
    "    results = {\n",
    "        'correct_selections': 0,\n",
    "        'total_selections': 0,\n",
    "        'total_sentences': 0,\n",
    "        'total_relevant': 0  # Added counter for relevant sentences\n",
    "    }\n",
    "    \n",
    "    for article, true_summary, article_preds in zip(articles, true_summaries, preds):\n",
    "        # Convert to string in case inputs aren't text\n",
    "        article = str(article)\n",
    "        true_summary = str(true_summary)\n",
    "        \n",
    "        sentences = nltk.sent_tokenize(article)[:MAX_SENTENCES]\n",
    "        summary_sentences = nltk.sent_tokenize(true_summary)\n",
    "        \n",
    "        # Create binary labels (1 if sentence is in summary)\n",
    "        true_labels = [1 if sent in true_summary else 0 for sent in sentences]\n",
    "        \n",
    "        # Pad to MAX_SENTENCES if needed\n",
    "        true_labels += [0] * (MAX_SENTENCES - len(true_labels))\n",
    "        \n",
    "        # Ensure predictions match length\n",
    "        article_preds = article_preds[:len(true_labels)]\n",
    "        \n",
    "        # Update counters\n",
    "        for true, pred in zip(true_labels, article_preds):\n",
    "            results['total_sentences'] += 1\n",
    "            if pred == 1:\n",
    "                results['total_selections'] += 1\n",
    "                if true == 1:\n",
    "                    results['correct_selections'] += 1\n",
    "            if true == 1:\n",
    "                results['total_relevant'] += 1\n",
    "    \n",
    "    # Calculate metrics with epsilon to avoid division by zero\n",
    "    eps = 1e-9\n",
    "    precision = results['correct_selections'] / (results['total_selections'] + eps)\n",
    "    recall = results['correct_selections'] / (results['total_relevant'] + eps)  # Fixed recall calculation\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "    \n",
    "    return {\n",
    "        'sentence_precision': precision,\n",
    "        'sentence_recall': recall,\n",
    "        'sentence_f1': f1,\n",
    "        'selection_accuracy': results['correct_selections'] / (results['total_sentences'] + eps),\n",
    "        'num_samples': len(articles)\n",
    "    }"
   ],
   "id": "16595d21eb520a47",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:32:56.867463Z",
     "start_time": "2025-03-29T15:32:56.563809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(predicted_summaries, reference_summaries):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = []\n",
    "    \n",
    "    for pred, ref in zip(predicted_summaries, reference_summaries):\n",
    "        if isinstance(pred, list):\n",
    "            pred = ' '.join(pred)\n",
    "        if isinstance(ref, list):\n",
    "            ref = ' '.join(ref)\n",
    "        scores.append(scorer.score(ref, pred))\n",
    "    \n",
    "    # Average scores\n",
    "    avg_scores = {\n",
    "        'rouge1': {'precision':0, 'recall':0, 'fmeasure':0},\n",
    "        'rouge2': {'precision':0, 'recall':0, 'fmeasure':0},\n",
    "        'rougeL': {'precision':0, 'recall':0, 'fmeasure':0}\n",
    "    }\n",
    "    \n",
    "    for score in scores:\n",
    "        for key in avg_scores:\n",
    "            for metric in ['precision', 'recall', 'fmeasure']:\n",
    "                avg_scores[key][metric] += score[key][metric]\n",
    "    \n",
    "    for key in avg_scores:\n",
    "        for metric in ['precision', 'recall', 'fmeasure']:\n",
    "            avg_scores[key][metric] /= len(scores)\n",
    "    \n",
    "    return avg_scores\n",
    "\n",
    "# Generate predicted summaries\n",
    "def generate_summaries(articles, preds):\n",
    "    summaries = []\n",
    "    for article, article_preds in zip(articles, preds):\n",
    "        sentences = nltk.sent_tokenize(article)[:MAX_SENTENCES]\n",
    "        selected = [sent for sent, pred in zip(sentences, article_preds[:len(sentences)]) if pred == 1]\n",
    "        summaries.append(' '.join(selected))\n",
    "    return summaries\n",
    "test_articles=dataset[\"test\"]['Article']\n",
    "test_summaries=dataset[\"test\"]['extractive_summary']\n",
    "final_preds=dataset[\"test\"]['extractive_summary']\n",
    "pred_summaries = generate_summaries(test_articles, final_preds)\n",
    "rouge_scores = calculate_rouge(pred_summaries, test_summaries)\n",
    "print(\"ROUGE scores:\", rouge_scores)"
   ],
   "id": "75d87f76e9578699",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 44\u001B[39m\n\u001B[32m     42\u001B[39m final_preds=dataset[\u001B[33m\"\u001B[39m\u001B[33mtest\u001B[39m\u001B[33m\"\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mextractive_summary\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     43\u001B[39m pred_summaries = generate_summaries(test_articles, final_preds)\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m rouge_scores = \u001B[43mcalculate_rouge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_summaries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_summaries\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mROUGE scores:\u001B[39m\u001B[33m\"\u001B[39m, rouge_scores)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 24\u001B[39m, in \u001B[36mcalculate_rouge\u001B[39m\u001B[34m(predicted_summaries, reference_summaries)\u001B[39m\n\u001B[32m     22\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m avg_scores:\n\u001B[32m     23\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m metric \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mprecision\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mrecall\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfmeasure\u001B[39m\u001B[33m'\u001B[39m]:\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m             avg_scores[key][metric] += \u001B[43mscore\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m avg_scores:\n\u001B[32m     27\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m metric \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mprecision\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mrecall\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfmeasure\u001B[39m\u001B[33m'\u001B[39m]:\n",
      "\u001B[31mTypeError\u001B[39m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:26:32.510459Z",
     "start_time": "2025-03-29T15:26:32.168635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metrics_dict):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "    \n",
    "    # Precision-Recall-F1\n",
    "    metrics = ['precision', 'recall', 'f1']\n",
    "    values = [metrics_dict[m] for m in metrics]\n",
    "    ax[0].bar(metrics, values)\n",
    "    ax[0].set_title('Sentence Classification Metrics')\n",
    "    ax[0].set_ylim(0,1)\n",
    "    \n",
    "    # ROUGE Scores\n",
    "    rouge_types = ['rouge1', 'rouge2', 'rougeL']\n",
    "    fmeasures = [metrics_dict['rouge'][t]['fmeasure'] for t in rouge_types]\n",
    "    ax[1].bar(rouge_types, fmeasures)\n",
    "    ax[1].set_title('ROUGE F1 Scores')\n",
    "    ax[1].set_ylim(0,1)\n",
    "    \n",
    "    # Selection Distribution\n",
    "    ax[2].pie(\n",
    "        [metrics_dict['correct_selections'], \n",
    "        metrics_dict['incorrect_selections']],\n",
    "        labels=['Correct', 'Incorrect'],\n",
    "        autopct='%1.1f%%'\n",
    "    )\n",
    "    ax[2].set_title('Selection Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Combine all metrics\n",
    "full_metrics = {\n",
    "    **metrics,\n",
    "    'rouge': rouge_scores,\n",
    "    'correct_selections': results['correct_selections'],\n",
    "    'incorrect_selections': results['total_selections'] - results['correct_selections']\n",
    "}\n",
    "plot_metrics(full_metrics)"
   ],
   "id": "ee4657aa22546bb1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[52]\u001B[39m\u001B[32m, line 34\u001B[39m\n\u001B[32m     30\u001B[39m     plt.show()\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# Combine all metrics\u001B[39;00m\n\u001B[32m     33\u001B[39m full_metrics = {\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     **\u001B[43mmetrics\u001B[49m,\n\u001B[32m     35\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mrouge\u001B[39m\u001B[33m'\u001B[39m: rouge_scores,\n\u001B[32m     36\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mcorrect_selections\u001B[39m\u001B[33m'\u001B[39m: results[\u001B[33m'\u001B[39m\u001B[33mcorrect_selections\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m     37\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mincorrect_selections\u001B[39m\u001B[33m'\u001B[39m: results[\u001B[33m'\u001B[39m\u001B[33mtotal_selections\u001B[39m\u001B[33m'\u001B[39m] - results[\u001B[33m'\u001B[39m\u001B[33mcorrect_selections\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     38\u001B[39m }\n\u001B[32m     39\u001B[39m plot_metrics(full_metrics)\n",
      "\u001B[31mNameError\u001B[39m: name 'metrics' is not defined"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b58acc1cf7b3edde"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
